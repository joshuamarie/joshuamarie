[
  {
    "objectID": "services.html",
    "href": "services.html",
    "title": "Want my help?",
    "section": "",
    "text": "To join is simple: Complete the form by clicking the hyperlinked text\n\n\nCore of R programming: Lectures about the heart of R programming, and it has 3 courses.\nCore of Python programming (not available right now): Lectures about the heart of R programming\nBayesianism (not available right now): Stan probabilistic programming is the main course.\n\n\n\n\n\nEmail me at: joshua.marie.k@gmail.com if you want to reach me\n\n\nData Manipulation\nData Visualization\nCalculus & Linear Algebra\nStatistical Modelling & Machine Learning\nResearch & Development\n\n\nFurther instruction: Here is the template subject to make a deal: “Consultation: Data Manipulation”"
  },
  {
    "objectID": "services.html#tutorials",
    "href": "services.html#tutorials",
    "title": "Want my help?",
    "section": "",
    "text": "To join is simple: Complete the form by clicking the hyperlinked text\n\n\nCore of R programming: Lectures about the heart of R programming, and it has 3 courses.\nCore of Python programming (not available right now): Lectures about the heart of R programming\nBayesianism (not available right now): Stan probabilistic programming is the main course."
  },
  {
    "objectID": "services.html#consultancy",
    "href": "services.html#consultancy",
    "title": "Want my help?",
    "section": "",
    "text": "Email me at: joshua.marie.k@gmail.com if you want to reach me\n\n\nData Manipulation\nData Visualization\nCalculus & Linear Algebra\nStatistical Modelling & Machine Learning\nResearch & Development\n\n\nFurther instruction: Here is the template subject to make a deal: “Consultation: Data Manipulation”"
  },
  {
    "objectID": "posts/02-arima-grid-search/index.html",
    "href": "posts/02-arima-grid-search/index.html",
    "title": "First level of time series modelling: Basic ARIMA model hyperparameter tuning",
    "section": "",
    "text": "How do you train ARIMA model in your time series models? Grid search, or Hyndman and Khandaka (2008) algorithm? I created this document to demonstrate you how to fit every possible ARIMA models using grid search with visualization. This is a basic problem of hyperparameter tuning. I prepare a simulated time series, and visualize the fitted values of every possible ARIMA models with ‘ggplot2’ and make it interactive with ggiraph.\n\n\nThe ARIMA (AutoRegressive Integrated Moving Average) model is defined by three parameters:\n\n\np: Autoregressive order that counts the past lagged terms (This is AR in ARIMA context)\n\nd: Differencing order that counts the number of differencing to achieve stationarity (This is ‘I’ or “integrate” in ARIMA context)\n\nq: Moving average order that counts the past error lagged terms (MA)\n\nChoosing the right combination of (p, d, q) is…not that easy, right when you want to achieve the best fit, even with Hyndman and Khandaka (2008) methodology with their forecast::auto.arima().\nThis is how it’s done:\n\nPrepare a time series data. I generate a time series data from this in order for you to replicate this.\n\nThen fit every possible ARIMA models across a grid of (p, d, q) values.\n\nThen evaluate the models performance by calculating the maximum log-likelihood then weight them with AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion).\n\nThen visualize the fitted values from every possible models, alongside the actual data.\n\nFrom the visual, highlight the best model in red.\n\nOptionally, you can make it interactive, using ‘ggiraph’, and I prepare it so that you can hover and explore model fits.\n\nThe packages used:\n\nbox (v1.2.0)\nggplot2 (v4.0.0)\nggiraph (v0.9.1)\npurrr (v1.0.2)\ndplyr (v1.1.4)\nforecast (v8.23.0)\nglue (v1.7.0)\ntidyr (v1.3.1)\nrlang (v1.1.4)\nscales (v1.4.0)\n\n\nWe generate a synthetic dataset with some trend and randomness:\n\nset.seed(123)\nts_sim = runif(365, 5, 10) + seq(-140, 224)^2 / 10000\nday = as.Date(\"2025-06-14\") - 0:364\n\nThis produces 365 daily observations with both trend and noise, which is a good test case for ARIMA.\n\nWe test a grid of ARIMA parameters:\n\\[\n\\begin{aligned}\np &\\in \\{0,1,2\\} \\\\\nd &\\in \\{0,1\\} \\\\\nq &\\in \\{0,1,2\\}\n\\end{aligned}\n\\]\nWe exclude overly complex models where p + q &gt; 3.\n\nCodemodels = local({\n    box::use(\n        purrr[pmap, pmap_chr, possibly, map, map_dbl], \n        dplyr[transmute, mutate, filter, slice_min, slice, case_when], \n        forecast[Arima], \n        glue[glue], \n        tidyr[expand_grid], \n        rlang[exec]\n    )\n    \n    expand_grid(p = 0:2, d = 0:1, q = 0:2) |&gt; \n        transmute(\n            models = pmap_chr(\n                pick(1:3), \n                \\(p, d, q) glue(\"ARIMA({p},{d},{q})\")\n            ), \n            res = pmap(\n                pick(1:3),\n                possibly(\n                    function (p, d, q) {\n                        if (p + q &gt; 3) return(NULL)\n                        exec(Arima, as.ts(ts_sim), order = c(p, d, q))\n                    },\n                    otherwise = NULL\n                )\n            ), \n            fits = map(res, ~ if(is.null(.x)) NULL else fitted(.x)),\n            aic = map_dbl(res, ~ if(is.null(.x)) NA_real_ else AIC(.x)),\n            bic = map_dbl(res, ~ if(is.null(.x)) NA_real_ else BIC(.x))\n        ) |&gt; \n        filter(!is.na(aic)) |&gt;  # Remove failed models\n        mutate(\n            day = list(day),\n            is_lowest_aic = aic == min(aic, na.rm = TRUE),\n            is_lowest_bic = bic == min(bic, na.rm = TRUE)\n        )\n})\nmodels\n\n\n  \n\n\n\nThis gives us a nested data frame of fitted models with their AIC, BIC, and fitted values.\n\nDo you want to visualize everything better, including the actual, fitted values, and highlight the fitted values made by the best fit?\nFrom models data, we just need the is_lowest_aic and is_lowest_bic. We just need to tweak the data a little bit here by expanding the fitted values with its corresponding data value. Then, set the model_type to condition the plotting data with dplyr::case_when().\n\nplot_data = local({\n    box::use(\n        tidyr[unnest], \n        dplyr[mutate, case_when]\n    )\n    models |&gt; \n        unnest(cols = c(day, fits)) |&gt;\n        mutate(\n            model_type = case_when(\n                is_lowest_aic ~ \"Best AIC\",\n                is_lowest_bic ~ \"Best BIC\", \n                TRUE ~ \"Other Models\"\n            )\n        )\n})\n\nOptionally:\n\n\nYou can put the information about the model which had the lowest AIC and BIC in an annotated box text in the plot.\n\nbest_model_1 = models |&gt; dplyr::filter(is_lowest_aic)\nbest_model_2 = models |&gt; dplyr::filter(is_lowest_bic)\n\n\n\nPointing out the maximum value of the time series data\n\nmax_val = max(ts_sim)\nmax_idx = which.max(ts_sim)\nmax_day = day[max_idx]\n\n\n\nThen, visualize:\n\nCodelocal({\n    box::use(\n        ggplot2[...],\n        scales[comma],\n        dplyr[filter], \n        glue[glue], \n    )\n    \n    p = ggplot() + \n        # Original data\n        geom_line(\n            aes(x = day, y = as.numeric(ts_sim)), \n            linewidth = 1.2, \n            color = \"#2C3E50\",\n            alpha = 0.8\n        ) + \n        geom_point(\n            aes(x = day, y = as.numeric(ts_sim)), \n            size = 0.8, \n            color = \"#2C3E50\", \n            alpha = 0.6\n        ) + \n        \n        # Other fitted models (light gray)\n        geom_line(\n            data = filter(plot_data, model_type == \"Other Models\"), \n            aes(x = day, y = fits, group = models), \n            color = '#BDC3C7',\n            alpha = 0.6,\n            linewidth = 0.5\n        ) + \n        \n        # Best BIC model (blue)\n        geom_line(\n            data = filter(plot_data, model_type == \"Best BIC\"), \n            aes(x = day, y = fits, group = models), \n            color = '#3498DB',\n            linewidth = 1.2,\n            alpha = 0.9\n        ) + \n        \n        # Best AIC model (red, on top)\n        geom_line(\n            data = filter(plot_data, model_type == \"Best AIC\"), \n            aes(x = day, y = fits, group = models), \n            color = '#E74C3C',\n            linewidth = 1.5,\n            alpha = 0.9\n        ) + \n        \n        # Peak annotation\n        annotate(\n            \"point\",\n            x = max_day, y = max_val,\n            size = 4, colour = \"#E74C3C\", shape = 21, stroke = 2, fill = \"white\"\n        ) + \n        annotate(\n            \"text\", \n            x = max_day + 25,\n            y = max_val + 0.5,\n            label = paste0(\"Peak: \", round(max_val, 1), \" sec\"), \n            color = \"#E74C3C\",\n            fontface = \"bold\",\n            size = 3.5\n        ) +\n        annotate(\n            \"curve\", \n            x = max_day + 20, xend = max_day + 0.1, \n            y = max_val + 0.3, yend = max_val, \n            linewidth = 0.8,\n            color = \"#E74C3C\", \n            curvature = -0.2,\n            arrow = arrow(length = unit(0.15, \"cm\"), type = \"closed\")\n        ) +\n        \n        # Model performance text box\n        annotate(\n            \"rect\",\n            xmin = min(day) + 20, xmax = min(day) + 100,\n            ymin = max(ts_sim) - 1.5, ymax = max(ts_sim) - 0.2,\n            fill = \"white\", color = \"#34495E\", alpha = 0.9\n        ) +\n        annotate(\n            \"text\",\n            x = min(day) + 60, y = max(ts_sim) - 0.5,\n            label = paste0(\"Best AIC: \", best_model_1$models, \n                           \"\\nAIC = \", round(best_model_1$aic, 1)),\n            color = \"#E74C3C\", fontface = \"bold\", size = 3.2\n        ) +\n        annotate(\n            \"text\",\n            x = min(day) + 60, y = max(ts_sim) - 1.1,\n            label = paste0(\"Best BIC: \", best_model_2$models,\n                           \"\\nBIC = \", round(best_model_2$bic, 1)),\n            color = \"#3498DB\", fontface = \"bold\", size = 3.2\n        ) +\n        \n        # Styling\n        scale_x_date(date_labels = \"%b %Y\", date_breaks = \"2 months\") +\n        scale_y_continuous(labels = comma) +\n        \n        labs(\n            x = \"Time Index\", \n            y = \"Simulated Response Values\", \n            title = \"ARIMA Model Grid Search: Simulated Time Series Analysis\",\n            subtitle = glue(\"Comparing {nrow(models)} successful ARIMA models • Best performing models highlighted\"),\n            caption = paste0(\"Models tested: p∈[0,2], d∈[0,1], q∈[0,2] • \",\n                             \"Original data shown in dark gray • \",\n                             \"Total observations: \", length(ts_sim))\n        ) + \n        \n        theme_minimal(base_size = 11, base_family = \"serif\") +\n        theme(\n            plot.title = element_text(\n                family = \"serif\", \n                colour = \"#2C3E50\",\n                size = 14,\n                face = \"bold\",\n                margin = margin(b = 5)\n            ),\n            plot.subtitle = element_text(\n                family = \"serif\",\n                colour = \"#7F8C8D\",\n                size = 10,\n                margin = margin(b = 15)\n            ),\n            plot.caption = element_text(\n                family = \"serif\",\n                colour = \"#95A5A6\",\n                size = 8,\n                margin = margin(t = 10)\n            ),\n            axis.text.x = element_text(\n                angle = 45, \n                hjust = 1,\n                margin = margin(t = 8),\n                color = \"#34495E\"\n            ), \n            axis.text.y = element_text(\n                margin = margin(r = 8),\n                color = \"#34495E\"\n            ), \n            axis.title = element_text(\n                color = \"#2C3E50\",\n                face = \"bold\"\n            ),\n            panel.grid.minor = element_blank(),\n            panel.grid.major = element_line(\n                color = \"#ECF0F1\", \n                linewidth = 0.3\n            ), \n            panel.background = element_rect(fill = \"#FEFEFE\", color = NA),\n            plot.background = element_rect(fill = \"white\", color = NA),\n            plot.margin = margin(20, 20, 20, 20)\n        ) \n    \n    p\n})\n\n\n\n\n\n\n\nWe visualize:\n\nOriginal data (dark gray)\nAll fitted values from every possible ARIMA model (light gray), except, the fitted values from the best fit is highlighted (red, blue, based on AIC, BIC, respectively)\nAnnotated peak point in the data\nAnnotated best AIC/BIC values\n\n\nIf you prefer your plot to be interactive like some figures in the website, use ‘ggiraph’ interactive interface version of ggplot2, then girafe(). The output produced by girafe() is wrapped with HTML, so it can be run in web.\nI recommend ‘ggiraph’ to build web applications in R.\nThis is the interactive version of the plot above:\n\nCodelocal({\n    box::use(\n        ggplot2[...],\n        scales[comma],\n        dplyr[filter, mutate, case_when], \n        glue[glue], \n        ggiraph[geom_line_interactive, geom_point_interactive, girafe, opts_hover, opts_hover_inv, opts_selection], \n        tidyr[unnest]\n    )\n    \n    # Use this for preparation\n    original_data = data.frame(\n        day = day,\n        ts_sim = ts_sim,\n        tooltip_line = \"actual data\",\n        tooltip_point = paste0(format(day, \"%Y-%m-%d\"), \"; readings: \", round(ts_sim, 1), \" secs\")\n    )\n    \n    plot_data = models |&gt; \n        unnest(cols = c(day, fits)) |&gt;\n        mutate(\n            model_type = case_when(\n                is_lowest_aic ~ \"Best AIC\",\n                is_lowest_bic ~ \"Best BIC\", \n                TRUE ~ \"Other Models\"\n            ),\n            # Create tooltip text for model lines\n            tooltip_text = case_when(\n                is_lowest_aic ~ paste0(\"best model: \", models, \"; AIC = \", round(aic, 2), \"; BIC = \", round(bic, 2)),\n                is_lowest_bic ~ paste0(\"best model: \", models, \"; AIC = \", round(aic, 2), \"; BIC = \", round(bic, 2)),\n                TRUE ~ paste0(\"model: \", models, \"; AIC = \", round(aic, 2), \"; BIC = \", round(bic, 2))\n            )\n        )\n  \n    p = ggplot() + \n        geom_line_interactive(\n            data = original_data,\n            aes(x = day, y = ts_sim, tooltip = tooltip_line, data_id = \"original_line\"), \n            linewidth = 1.2, \n            color = \"#2C3E50\",\n            alpha = 0.8\n        ) + \n        geom_point_interactive(\n            data = original_data,\n            aes(x = day, y = ts_sim, tooltip = tooltip_point), \n            size = 0.8, \n            color = \"#2C3E50\", \n            alpha = 0.6\n        ) + \n        \n        geom_line_interactive(\n            data = filter(plot_data, model_type == \"Other Models\"), \n            aes(x = day, y = fits, group = models, tooltip = tooltip_text, data_id = models), \n            color = '#BDC3C7',\n            alpha = 0.6,\n            linewidth = 0.5\n        ) + \n        \n        geom_line_interactive(\n            data = filter(plot_data, model_type == \"Best BIC\"), \n            aes(x = day, y = fits, group = models, tooltip = tooltip_text, data_id = models), \n            color = '#3498DB',\n            linewidth = 1.2,\n            alpha = 0.9\n        ) + \n        \n        geom_line_interactive(\n            data = filter(plot_data, model_type == \"Best AIC\"), \n            aes(x = day, y = fits, group = models, tooltip = tooltip_text, data_id = models), \n            color = '#E74C3C',\n            linewidth = 1.5,\n            alpha = 0.9\n        ) + \n        \n        scale_x_date(date_labels = \"%b %Y\", date_breaks = \"2 months\") +\n        scale_y_continuous(labels = comma) +\n        \n        labs(\n            x = \"Time Index\", \n            y = \"Simulated Response Values\", \n            title = \"ARIMA Model Grid Search: Simulated Time Series Analysis\",\n            subtitle = glue(\"Comparing {nrow(models)} successful ARIMA models • Best performing models highlighted\"),\n            caption = paste0(\"Models tested: p∈[0,2], d∈[0,1], q∈[0,2] • \",\n                             \"Original data shown in dark gray • \",\n                             \"Total observations: \", length(ts_sim))\n        ) + \n        \n        theme_minimal(base_size = 11, base_family = \"serif\") +\n        theme(\n            plot.title = element_text(\n                family = \"serif\", \n                colour = \"#2C3E50\",\n                size = 14,\n                face = \"bold\",\n                margin = margin(b = 5)\n            ),\n            plot.subtitle = element_text(\n                family = \"serif\",\n                colour = \"#7F8C8D\",\n                size = 10,\n                margin = margin(b = 15)\n            ),\n            plot.caption = element_text(\n                family = \"serif\",\n                colour = \"#95A5A6\",\n                size = 8,\n                margin = margin(t = 10)\n            ),\n            axis.text.x = element_text(\n                angle = 45, \n                hjust = 1,\n                margin = margin(t = 8),\n                color = \"#34495E\"\n            ), \n            axis.text.y = element_text(\n                margin = margin(r = 8),\n                color = \"#34495E\"\n            ), \n            axis.title = element_text(\n                color = \"#2C3E50\",\n                face = \"bold\"\n            ),\n            panel.grid.minor = element_blank(),\n            panel.grid.major = element_line(\n                color = \"#ECF0F1\", \n                linewidth = 0.3\n            ), \n            panel.background = element_rect(fill = \"#FEFEFE\", color = NA),\n            plot.background = element_rect(fill = \"white\", color = NA),\n            plot.margin = margin(20, 20, 20, 20)\n        ) \n    \n    interactive_plot = girafe(\n        ggobj = p,\n        options = list(\n            opts_hover(css = \"cursor:pointer;stroke-width:4;stroke-opacity:1;fill-opacity:1;r:4px;\"),\n            opts_hover_inv(css = \"opacity:0.1;\"),\n            opts_selection(type = \"none\")\n        )\n    )\n    \n    interactive_plot\n})\n\n\n\n\n\n\nThis is just a toy example of leveraging functional programming and basic hyperparameter tuning for time series in R, and some of my learning competencies about data visualization in R and how to get deeper in it.\nIf you are interested to learn more, check out my other gists."
  },
  {
    "objectID": "posts/02-arima-grid-search/index.html#introduction",
    "href": "posts/02-arima-grid-search/index.html#introduction",
    "title": "First level of time series modelling: Basic ARIMA model hyperparameter tuning",
    "section": "",
    "text": "The ARIMA (AutoRegressive Integrated Moving Average) model is defined by three parameters:\n\n\np: Autoregressive order that counts the past lagged terms (This is AR in ARIMA context)\n\nd: Differencing order that counts the number of differencing to achieve stationarity (This is ‘I’ or “integrate” in ARIMA context)\n\nq: Moving average order that counts the past error lagged terms (MA)\n\nChoosing the right combination of (p, d, q) is…not that easy, right when you want to achieve the best fit, even with Hyndman and Khandaka (2008) methodology with their forecast::auto.arima().\nThis is how it’s done:\n\nPrepare a time series data. I generate a time series data from this in order for you to replicate this.\n\nThen fit every possible ARIMA models across a grid of (p, d, q) values.\n\nThen evaluate the models performance by calculating the maximum log-likelihood then weight them with AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion).\n\nThen visualize the fitted values from every possible models, alongside the actual data.\n\nFrom the visual, highlight the best model in red.\n\nOptionally, you can make it interactive, using ‘ggiraph’, and I prepare it so that you can hover and explore model fits.\n\nThe packages used:\n\nbox (v1.2.0)\nggplot2 (v4.0.0)\nggiraph (v0.9.1)\npurrr (v1.0.2)\ndplyr (v1.1.4)\nforecast (v8.23.0)\nglue (v1.7.0)\ntidyr (v1.3.1)\nrlang (v1.1.4)\nscales (v1.4.0)"
  },
  {
    "objectID": "posts/02-arima-grid-search/index.html#simulating-data",
    "href": "posts/02-arima-grid-search/index.html#simulating-data",
    "title": "First level of time series modelling: Basic ARIMA model hyperparameter tuning",
    "section": "",
    "text": "We generate a synthetic dataset with some trend and randomness:\n\nset.seed(123)\nts_sim = runif(365, 5, 10) + seq(-140, 224)^2 / 10000\nday = as.Date(\"2025-06-14\") - 0:364\n\nThis produces 365 daily observations with both trend and noise, which is a good test case for ARIMA."
  },
  {
    "objectID": "posts/02-arima-grid-search/index.html#fitting-multiple-arima-models",
    "href": "posts/02-arima-grid-search/index.html#fitting-multiple-arima-models",
    "title": "First level of time series modelling: Basic ARIMA model hyperparameter tuning",
    "section": "",
    "text": "We test a grid of ARIMA parameters:\n\\[\n\\begin{aligned}\np &\\in \\{0,1,2\\} \\\\\nd &\\in \\{0,1\\} \\\\\nq &\\in \\{0,1,2\\}\n\\end{aligned}\n\\]\nWe exclude overly complex models where p + q &gt; 3.\n\nCodemodels = local({\n    box::use(\n        purrr[pmap, pmap_chr, possibly, map, map_dbl], \n        dplyr[transmute, mutate, filter, slice_min, slice, case_when], \n        forecast[Arima], \n        glue[glue], \n        tidyr[expand_grid], \n        rlang[exec]\n    )\n    \n    expand_grid(p = 0:2, d = 0:1, q = 0:2) |&gt; \n        transmute(\n            models = pmap_chr(\n                pick(1:3), \n                \\(p, d, q) glue(\"ARIMA({p},{d},{q})\")\n            ), \n            res = pmap(\n                pick(1:3),\n                possibly(\n                    function (p, d, q) {\n                        if (p + q &gt; 3) return(NULL)\n                        exec(Arima, as.ts(ts_sim), order = c(p, d, q))\n                    },\n                    otherwise = NULL\n                )\n            ), \n            fits = map(res, ~ if(is.null(.x)) NULL else fitted(.x)),\n            aic = map_dbl(res, ~ if(is.null(.x)) NA_real_ else AIC(.x)),\n            bic = map_dbl(res, ~ if(is.null(.x)) NA_real_ else BIC(.x))\n        ) |&gt; \n        filter(!is.na(aic)) |&gt;  # Remove failed models\n        mutate(\n            day = list(day),\n            is_lowest_aic = aic == min(aic, na.rm = TRUE),\n            is_lowest_bic = bic == min(bic, na.rm = TRUE)\n        )\n})\nmodels\n\n\n  \n\n\n\nThis gives us a nested data frame of fitted models with their AIC, BIC, and fitted values."
  },
  {
    "objectID": "posts/02-arima-grid-search/index.html#visualizing-models-with-ggplot2",
    "href": "posts/02-arima-grid-search/index.html#visualizing-models-with-ggplot2",
    "title": "First level of time series modelling: Basic ARIMA model hyperparameter tuning",
    "section": "",
    "text": "Do you want to visualize everything better, including the actual, fitted values, and highlight the fitted values made by the best fit?\nFrom models data, we just need the is_lowest_aic and is_lowest_bic. We just need to tweak the data a little bit here by expanding the fitted values with its corresponding data value. Then, set the model_type to condition the plotting data with dplyr::case_when().\n\nplot_data = local({\n    box::use(\n        tidyr[unnest], \n        dplyr[mutate, case_when]\n    )\n    models |&gt; \n        unnest(cols = c(day, fits)) |&gt;\n        mutate(\n            model_type = case_when(\n                is_lowest_aic ~ \"Best AIC\",\n                is_lowest_bic ~ \"Best BIC\", \n                TRUE ~ \"Other Models\"\n            )\n        )\n})\n\nOptionally:\n\n\nYou can put the information about the model which had the lowest AIC and BIC in an annotated box text in the plot.\n\nbest_model_1 = models |&gt; dplyr::filter(is_lowest_aic)\nbest_model_2 = models |&gt; dplyr::filter(is_lowest_bic)\n\n\n\nPointing out the maximum value of the time series data\n\nmax_val = max(ts_sim)\nmax_idx = which.max(ts_sim)\nmax_day = day[max_idx]\n\n\n\nThen, visualize:\n\nCodelocal({\n    box::use(\n        ggplot2[...],\n        scales[comma],\n        dplyr[filter], \n        glue[glue], \n    )\n    \n    p = ggplot() + \n        # Original data\n        geom_line(\n            aes(x = day, y = as.numeric(ts_sim)), \n            linewidth = 1.2, \n            color = \"#2C3E50\",\n            alpha = 0.8\n        ) + \n        geom_point(\n            aes(x = day, y = as.numeric(ts_sim)), \n            size = 0.8, \n            color = \"#2C3E50\", \n            alpha = 0.6\n        ) + \n        \n        # Other fitted models (light gray)\n        geom_line(\n            data = filter(plot_data, model_type == \"Other Models\"), \n            aes(x = day, y = fits, group = models), \n            color = '#BDC3C7',\n            alpha = 0.6,\n            linewidth = 0.5\n        ) + \n        \n        # Best BIC model (blue)\n        geom_line(\n            data = filter(plot_data, model_type == \"Best BIC\"), \n            aes(x = day, y = fits, group = models), \n            color = '#3498DB',\n            linewidth = 1.2,\n            alpha = 0.9\n        ) + \n        \n        # Best AIC model (red, on top)\n        geom_line(\n            data = filter(plot_data, model_type == \"Best AIC\"), \n            aes(x = day, y = fits, group = models), \n            color = '#E74C3C',\n            linewidth = 1.5,\n            alpha = 0.9\n        ) + \n        \n        # Peak annotation\n        annotate(\n            \"point\",\n            x = max_day, y = max_val,\n            size = 4, colour = \"#E74C3C\", shape = 21, stroke = 2, fill = \"white\"\n        ) + \n        annotate(\n            \"text\", \n            x = max_day + 25,\n            y = max_val + 0.5,\n            label = paste0(\"Peak: \", round(max_val, 1), \" sec\"), \n            color = \"#E74C3C\",\n            fontface = \"bold\",\n            size = 3.5\n        ) +\n        annotate(\n            \"curve\", \n            x = max_day + 20, xend = max_day + 0.1, \n            y = max_val + 0.3, yend = max_val, \n            linewidth = 0.8,\n            color = \"#E74C3C\", \n            curvature = -0.2,\n            arrow = arrow(length = unit(0.15, \"cm\"), type = \"closed\")\n        ) +\n        \n        # Model performance text box\n        annotate(\n            \"rect\",\n            xmin = min(day) + 20, xmax = min(day) + 100,\n            ymin = max(ts_sim) - 1.5, ymax = max(ts_sim) - 0.2,\n            fill = \"white\", color = \"#34495E\", alpha = 0.9\n        ) +\n        annotate(\n            \"text\",\n            x = min(day) + 60, y = max(ts_sim) - 0.5,\n            label = paste0(\"Best AIC: \", best_model_1$models, \n                           \"\\nAIC = \", round(best_model_1$aic, 1)),\n            color = \"#E74C3C\", fontface = \"bold\", size = 3.2\n        ) +\n        annotate(\n            \"text\",\n            x = min(day) + 60, y = max(ts_sim) - 1.1,\n            label = paste0(\"Best BIC: \", best_model_2$models,\n                           \"\\nBIC = \", round(best_model_2$bic, 1)),\n            color = \"#3498DB\", fontface = \"bold\", size = 3.2\n        ) +\n        \n        # Styling\n        scale_x_date(date_labels = \"%b %Y\", date_breaks = \"2 months\") +\n        scale_y_continuous(labels = comma) +\n        \n        labs(\n            x = \"Time Index\", \n            y = \"Simulated Response Values\", \n            title = \"ARIMA Model Grid Search: Simulated Time Series Analysis\",\n            subtitle = glue(\"Comparing {nrow(models)} successful ARIMA models • Best performing models highlighted\"),\n            caption = paste0(\"Models tested: p∈[0,2], d∈[0,1], q∈[0,2] • \",\n                             \"Original data shown in dark gray • \",\n                             \"Total observations: \", length(ts_sim))\n        ) + \n        \n        theme_minimal(base_size = 11, base_family = \"serif\") +\n        theme(\n            plot.title = element_text(\n                family = \"serif\", \n                colour = \"#2C3E50\",\n                size = 14,\n                face = \"bold\",\n                margin = margin(b = 5)\n            ),\n            plot.subtitle = element_text(\n                family = \"serif\",\n                colour = \"#7F8C8D\",\n                size = 10,\n                margin = margin(b = 15)\n            ),\n            plot.caption = element_text(\n                family = \"serif\",\n                colour = \"#95A5A6\",\n                size = 8,\n                margin = margin(t = 10)\n            ),\n            axis.text.x = element_text(\n                angle = 45, \n                hjust = 1,\n                margin = margin(t = 8),\n                color = \"#34495E\"\n            ), \n            axis.text.y = element_text(\n                margin = margin(r = 8),\n                color = \"#34495E\"\n            ), \n            axis.title = element_text(\n                color = \"#2C3E50\",\n                face = \"bold\"\n            ),\n            panel.grid.minor = element_blank(),\n            panel.grid.major = element_line(\n                color = \"#ECF0F1\", \n                linewidth = 0.3\n            ), \n            panel.background = element_rect(fill = \"#FEFEFE\", color = NA),\n            plot.background = element_rect(fill = \"white\", color = NA),\n            plot.margin = margin(20, 20, 20, 20)\n        ) \n    \n    p\n})\n\n\n\n\n\n\n\nWe visualize:\n\nOriginal data (dark gray)\nAll fitted values from every possible ARIMA model (light gray), except, the fitted values from the best fit is highlighted (red, blue, based on AIC, BIC, respectively)\nAnnotated peak point in the data\nAnnotated best AIC/BIC values"
  },
  {
    "objectID": "posts/02-arima-grid-search/index.html#optional-interactive-visualization-with-ggiraph",
    "href": "posts/02-arima-grid-search/index.html#optional-interactive-visualization-with-ggiraph",
    "title": "First level of time series modelling: Basic ARIMA model hyperparameter tuning",
    "section": "",
    "text": "If you prefer your plot to be interactive like some figures in the website, use ‘ggiraph’ interactive interface version of ggplot2, then girafe(). The output produced by girafe() is wrapped with HTML, so it can be run in web.\nI recommend ‘ggiraph’ to build web applications in R.\nThis is the interactive version of the plot above:\n\nCodelocal({\n    box::use(\n        ggplot2[...],\n        scales[comma],\n        dplyr[filter, mutate, case_when], \n        glue[glue], \n        ggiraph[geom_line_interactive, geom_point_interactive, girafe, opts_hover, opts_hover_inv, opts_selection], \n        tidyr[unnest]\n    )\n    \n    # Use this for preparation\n    original_data = data.frame(\n        day = day,\n        ts_sim = ts_sim,\n        tooltip_line = \"actual data\",\n        tooltip_point = paste0(format(day, \"%Y-%m-%d\"), \"; readings: \", round(ts_sim, 1), \" secs\")\n    )\n    \n    plot_data = models |&gt; \n        unnest(cols = c(day, fits)) |&gt;\n        mutate(\n            model_type = case_when(\n                is_lowest_aic ~ \"Best AIC\",\n                is_lowest_bic ~ \"Best BIC\", \n                TRUE ~ \"Other Models\"\n            ),\n            # Create tooltip text for model lines\n            tooltip_text = case_when(\n                is_lowest_aic ~ paste0(\"best model: \", models, \"; AIC = \", round(aic, 2), \"; BIC = \", round(bic, 2)),\n                is_lowest_bic ~ paste0(\"best model: \", models, \"; AIC = \", round(aic, 2), \"; BIC = \", round(bic, 2)),\n                TRUE ~ paste0(\"model: \", models, \"; AIC = \", round(aic, 2), \"; BIC = \", round(bic, 2))\n            )\n        )\n  \n    p = ggplot() + \n        geom_line_interactive(\n            data = original_data,\n            aes(x = day, y = ts_sim, tooltip = tooltip_line, data_id = \"original_line\"), \n            linewidth = 1.2, \n            color = \"#2C3E50\",\n            alpha = 0.8\n        ) + \n        geom_point_interactive(\n            data = original_data,\n            aes(x = day, y = ts_sim, tooltip = tooltip_point), \n            size = 0.8, \n            color = \"#2C3E50\", \n            alpha = 0.6\n        ) + \n        \n        geom_line_interactive(\n            data = filter(plot_data, model_type == \"Other Models\"), \n            aes(x = day, y = fits, group = models, tooltip = tooltip_text, data_id = models), \n            color = '#BDC3C7',\n            alpha = 0.6,\n            linewidth = 0.5\n        ) + \n        \n        geom_line_interactive(\n            data = filter(plot_data, model_type == \"Best BIC\"), \n            aes(x = day, y = fits, group = models, tooltip = tooltip_text, data_id = models), \n            color = '#3498DB',\n            linewidth = 1.2,\n            alpha = 0.9\n        ) + \n        \n        geom_line_interactive(\n            data = filter(plot_data, model_type == \"Best AIC\"), \n            aes(x = day, y = fits, group = models, tooltip = tooltip_text, data_id = models), \n            color = '#E74C3C',\n            linewidth = 1.5,\n            alpha = 0.9\n        ) + \n        \n        scale_x_date(date_labels = \"%b %Y\", date_breaks = \"2 months\") +\n        scale_y_continuous(labels = comma) +\n        \n        labs(\n            x = \"Time Index\", \n            y = \"Simulated Response Values\", \n            title = \"ARIMA Model Grid Search: Simulated Time Series Analysis\",\n            subtitle = glue(\"Comparing {nrow(models)} successful ARIMA models • Best performing models highlighted\"),\n            caption = paste0(\"Models tested: p∈[0,2], d∈[0,1], q∈[0,2] • \",\n                             \"Original data shown in dark gray • \",\n                             \"Total observations: \", length(ts_sim))\n        ) + \n        \n        theme_minimal(base_size = 11, base_family = \"serif\") +\n        theme(\n            plot.title = element_text(\n                family = \"serif\", \n                colour = \"#2C3E50\",\n                size = 14,\n                face = \"bold\",\n                margin = margin(b = 5)\n            ),\n            plot.subtitle = element_text(\n                family = \"serif\",\n                colour = \"#7F8C8D\",\n                size = 10,\n                margin = margin(b = 15)\n            ),\n            plot.caption = element_text(\n                family = \"serif\",\n                colour = \"#95A5A6\",\n                size = 8,\n                margin = margin(t = 10)\n            ),\n            axis.text.x = element_text(\n                angle = 45, \n                hjust = 1,\n                margin = margin(t = 8),\n                color = \"#34495E\"\n            ), \n            axis.text.y = element_text(\n                margin = margin(r = 8),\n                color = \"#34495E\"\n            ), \n            axis.title = element_text(\n                color = \"#2C3E50\",\n                face = \"bold\"\n            ),\n            panel.grid.minor = element_blank(),\n            panel.grid.major = element_line(\n                color = \"#ECF0F1\", \n                linewidth = 0.3\n            ), \n            panel.background = element_rect(fill = \"#FEFEFE\", color = NA),\n            plot.background = element_rect(fill = \"white\", color = NA),\n            plot.margin = margin(20, 20, 20, 20)\n        ) \n    \n    interactive_plot = girafe(\n        ggobj = p,\n        options = list(\n            opts_hover(css = \"cursor:pointer;stroke-width:4;stroke-opacity:1;fill-opacity:1;r:4px;\"),\n            opts_hover_inv(css = \"opacity:0.1;\"),\n            opts_selection(type = \"none\")\n        )\n    )\n    \n    interactive_plot\n})"
  },
  {
    "objectID": "posts/02-arima-grid-search/index.html#disclaimer",
    "href": "posts/02-arima-grid-search/index.html#disclaimer",
    "title": "First level of time series modelling: Basic ARIMA model hyperparameter tuning",
    "section": "",
    "text": "This is just a toy example of leveraging functional programming and basic hyperparameter tuning for time series in R, and some of my learning competencies about data visualization in R and how to get deeper in it.\nIf you are interested to learn more, check out my other gists."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My name is Joshua",
    "section": "",
    "text": "Computer Scientist • Statistician\n\n\nI am a computer scientist, package creator / maintainer / contributor (to R, Python, Julia), and a statistician. I studied multiple layers of programming paradigms and designs, and still actively learning. From what I studied so far, R and Python have closely equivalent feature parities, hence I use both for data science-y activities :). I write packages for statistics, data science, and numerical analysis, as my specialties found in those fields.\n\nKnow About Me"
  },
  {
    "objectID": "index.html#joshua-marie",
    "href": "index.html#joshua-marie",
    "title": "My name is Joshua",
    "section": "",
    "text": "Computer Scientist • Statistician\n\n\nI am a computer scientist, package creator / maintainer / contributor (to R, Python, Julia), and a statistician. I studied multiple layers of programming paradigms and designs, and still actively learning. From what I studied so far, R and Python have closely equivalent feature parities, hence I use both for data science-y activities :). I write packages for statistics, data science, and numerical analysis, as my specialties found in those fields.\n\nKnow About Me"
  },
  {
    "objectID": "index.html#recent-posts",
    "href": "index.html#recent-posts",
    "title": "My name is Joshua",
    "section": "Recent Posts",
    "text": "Recent Posts\n\n\n\n\n\n\n\n\n\n\nHow much do you know about pipes?\n\n\nHistory and Functionality of pipes\n\n\n\nR\n\n\npipes\n\n\n\n\n\n\n\n\n\nNov 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nThe Hidden Magic of Tidy-Select: R’s Universal Column Selection Language\n\n\nNot limited to tidyselect helpers\n\n\n\n\n\n\n\n\nNov 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBox: Placing module system into R\n\n\nDefinitive Guide\n\n\n\nR\n\n\nbook\n\n\n\n\n\n\n\n\n\nOct 8, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nFirst level of time series modelling: Basic ARIMA model hyperparameter tuning\n\n\nWith plots\n\n\n\nR\n\n\ntime-series\n\n\nmachine-learning\n\n\n\n\n\n\n\n\n\nSep 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAutomatically generate Deep Feedforward Neural Network (DFFNN) module from torch expression\n\n\nUse case of code generation in practice\n\n\n\nR\n\n\nmachine-learning\n\n\ntorch\n\n\npointless-code\n\n\n\n\n\n\n\n\n\nSep 23, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "About Me - Card Layout\n    \n    \n    \n    \n    \n    \n    \n    \n\n    \n\n\n\n\n\n  \n   \n    \n      I'm a \n      \n        \n          Computer Scientist\n          Statistician\n          Data Scientist\n          Consultant\nHowdy! I’m your neighborhood statistics man — a proud statistics man with some passions for mathematics, statistics, and making sense of the chaos. I serve as a data whisperer, tutor, and a consultant. I am a (self-proclaimed) computer scientist, technical person, and a statistician. I studied layers of programming paradigms. From what I studied so far, R and Python have closely equivalent feature parities. I write packages for statistics, data science, and numerical analysis, as my specialties found in those fields."
  },
  {
    "objectID": "about.html#formal-education",
    "href": "about.html#formal-education",
    "title": "About Me",
    "section": "Formal Education",
    "text": "Formal Education"
  },
  {
    "objectID": "about.html#skillsets",
    "href": "about.html#skillsets",
    "title": "About Me",
    "section": "Skillsets",
    "text": "Skillsets"
  },
  {
    "objectID": "about.html#employment",
    "href": "about.html#employment",
    "title": "About Me",
    "section": "Employment",
    "text": "Employment\n\n\nStatistician @ Fixstat\nPackage maintainer"
  },
  {
    "objectID": "posts/01-meta-nn/index.html",
    "href": "posts/01-meta-nn/index.html",
    "title": "Automatically generate Deep Feedforward Neural Network (DFFNN) module from torch expression",
    "section": "",
    "text": "I have a course tutorial, which I discuss the things to get “advance” in R. Code generation is part of it. My blog compiles pointless codes in pointless code series, and this is one of them.\nNow, the question is: How do you define neural network architectures in your deep learning projects? Manually write out each layer? Copy-paste and modify existing code? In this part, I wanted to discuss it to you on how to leverage code generation technique that generates ‘torch’ neural network modules in a programmatic approach. This is a handy approach to building flexible, reusable neural network architectures without repetitive code.\nI’ll walk through a function that creates DFFNN expressions with customizable architectures, layer by layer, explaining each step along the way.\n\n\nThe create_nn_module() function dynamically generates torch neural network module definitions. Instead of manually writing out layer definitions and forward pass logic, this function builds the code expressions for you.\nKey benefits:\n\n\nFlexibility: Change network architecture with a single function call\n\nAutomation: Generate multiple network configurations programmatically\n\nExperimentation: Quickly test different architectures in hyperparameter searches\n\nThis is how it’s done:\n\nDefine the network architecture (input size, hidden layers, output size)\nSpecify activation functions for each layer\nProgrammatically generate the initialize method (layer definitions)\nProgrammatically generate the forward method (forward pass logic)\nReturn an nn_module expression ready to be evaluated\n\nThe packages used:\n\n\nrlang (v1.1.4) - For metaprogramming tools\n\npurrr (v1.0.2) - For functional programming\n\nglue (v1.7.0) - For string interpolation\n\nmagrittr - For pipe operator\n\nbox (v1.2.0) - For modular code organization\n\nI created create_nn_module() function a while ago and shared it on GitHub Gist. Here’s the function we’ll be analyzing:\n\nCodecreate_nn_module = function(nn_name = \"DeepFFN\", hd_neurons = c(20, 30, 20, 15), no_x = 10, no_y = 1, activations = NULL) {\n    box::use(\n        rlang[new_function, call2, caller_env, expr, exprs, sym, is_function, env_get_list],\n        purrr[map, map2, reduce, set_names, compact, map_if, keep, map_lgl], \n        glue[glue], \n        magrittr[`%&gt;%`]\n    )\n    \n    nodes = c(no_x, hd_neurons, no_y)\n    n_layers = length(nodes) - 1\n    \n    call_args = match.call()\n    activation_arg = call_args$activations\n    \n    if (is.null(activations)) {\n        activations = c(rep(\"nnf_relu\", length(hd_neurons)), NA)\n    } else if (length(activations) == 1 || is.function(activations)) {\n        single_activation = activations\n        activations = c(rep(list(single_activation), length(hd_neurons)), list(NA))\n    }\n    \n    activations = map2(activations, seq_along(activations), function(x, i) {\n        if (is.null(x)) {\n            NULL\n        } else if (is.function(x)) {\n            if(!is.null(activation_arg) && is.call(activation_arg) && activation_arg[[1]] == quote(c)) {\n                func_name = as.character(activation_arg[[i + 1]])\n                sym(func_name)\n            } else if(!is.null(activation_arg) && (is.symbol(activation_arg) || is.character(activation_arg))) {\n                func_name = as.character(activation_arg)\n                sym(func_name)\n            } else {\n                parent_env = parent.frame()\n                env_names = ls(envir = parent_env)\n                matching_names = env_names %&gt;%\n                    keep(~ {\n                        obj = env_get_list(parent_env, .x)[[1]]\n                        identical(obj, x)\n                    })\n                \n                if (length(matching_names) &gt; 0) {\n                    sym(matching_names[1])\n                } else {\n                    stop(\"Could not determine function name for activation function\")\n                }\n            }\n        } else if (is.character(x)) {\n            if (length(x) == 1 && is.na(x)) {\n                NULL\n            } else {\n                sym(x)\n            }\n        } else if (is.symbol(x)) {\n            x\n        } else if (is.logical(x) && length(x) == 1 && is.na(x)) {\n            NULL\n        } else {\n            stop(\"Activation must be a function, string, symbol, NA, or NULL\")\n        }\n    })\n    \n    init_body = map2(1:n_layers, map2(nodes[-length(nodes)], nodes[-1], c), function(i, dims) {\n        layer_name = if (i == n_layers) \"out\" else glue(\"fc{i}\")\n        call2(\"=\", call2(\"$\", expr(self), sym(layer_name)), call2(\"nn_linear\", !!!dims))\n    })\n    \n    init = new_function(\n        args = list(), \n        body = call2(\"{\", !!!init_body)\n    )\n    \n    layer_calls = map(1:n_layers, function(i) {\n        layer_name = if (i == n_layers) \"out\" else glue(\"fc{i}\")\n        activation_fn = if (i &lt;= length(activations)) activations[[i]] else NULL\n        \n        result = list(call2(call2(\"$\", expr(self), sym(layer_name))))\n        if (!is.null(activation_fn)) {\n            result = append(result, list(call2(activation_fn)))\n        }\n        result\n    }) |&gt; \n        unlist() |&gt; # recursive = FALSE is also valid\n        compact()\n    \n    forward_body = reduce(layer_calls, function(acc, call) {\n        expr(!!acc %&gt;% !!call)\n    }, .init = expr(x))\n    \n    forward = new_function(\n        args = list(x = expr()), \n        body = call2(\"{\", forward_body)\n    )\n    \n    call2(\"nn_module\", nn_name, initialize = init, forward = forward)\n}\n\n\n\n\nYou’ll notice that I’ve been using another approach to load namespace in R. But, why ‘box’? You need to check out my mini book dedicated on modular programming in R.\n\nWell, a function, or a function call, creates an environment, which encloses the objects and operations within it. In other words, we create a closure. This is actually a good practice for several reasons:\n\nNamespace isolation: Dependencies loaded inside the function will not make pollution the global environment, or conflicts with any packages loaded. When you load packages required with library(), inside a function or not, it attaches those packages to your search path, and will mask functions from other packages. With box::use() inside a function, the imports are scoped only to that function’s or call’s environment.\nExplicit dependencies: Anyone reading the function immediately knows what external tools it uses. You don’t have to scroll to the top of a script to see what’s loaded.\nReproducibility: The function becomes self-contained. If you share just this function, others know exactly what packages they need less hunting through documentation.\nAvoiding conflicts: Different functions can use different versions or implementations without conflicts. For example, one function might use dplyr::filter() while another uses stats::filter(), and they won’t interfere with each other.\nLazy loading: The packages are only loaded when the function is actually called, not when it’s defined. This can improve script startup time if you have many functions but only use a few.\n\n\n\n\n\n\n\nNote\n\n\n\nIn a nutshell: The ‘box’ package provides explicit, granular imports, making it transparent which namespace to be imported from which packages. It’s like having a well-organized toolbox where each tool is labeled.\n\n\n\nI’ll be trying to be concise on explaining each layers of the function so that you’ll understand what I did\n\nI use box::use() to load dependencies:\n\n\nrlang: Improvised Core R programming. One of the core R programming, metaprogramming which includes creating expressions and functions programmatically, are less painful than what base R have.\n\npurrr: Improvised Functional programming utilities.\n\nglue: R lacks Python’s f-string for string interpolation, although we have sprintf() and paste() for that. glue makes string interpolation more readable with glue(\"fc{i}\") instead of paste0(\"fc\", i) or sprintf(\"fc%d\", i).\n\nmagrittr: The pipe operator %&gt;% for chaining operations. This is optional, by the way — R 4.1+ has the native pipe |&gt;, but %&gt;% offers better flexibility with the dot placeholder.\n\nIn DFFNN architecture, it is defined by the input layer, the hidden layer, and the output layer.\n\n\nSource: https://medium.com/data-science/designing-your-neural-networks-a5e4617027ed\n\nThe number of nodes are defined by integers, except for input and output layer nodes which they are fixed and determined by the data you provided, and they are defined by no_x and no_y. The number of hidden layers is defined by the length of input in hd_neurons argument.\nCombine no_x, hd_neurons, no_y in order:\nnodes = c(no_x, hd_neurons, no_y)\nAnd then calculate the length of nodes, which is \\(1 + n_{\\text{hidden layres}} + 1\\), and then subtract it by 1 because the applied activation functions is invoked between each layer.\nn_layers = length(nodes) - 1\n\nWhen you have:\n\n10 predictors\n\n5 hidden layers, and for each layer:\n\n20 nodes\n30 nodes\n20 nodes\n15 nodes\n20 nodes\n\n\n1 response variable\n\nTotal number of layers: 7\nThis means we need 7 - 1 linear transformations, and here is my diagram:\n\\[10_{\\text{inputs}} \\rightarrow20_{\\text{hd1}} \\rightarrow30_{\\text{hd2}} \\rightarrow20_{\\text{hd3}} \\rightarrow15_{\\text{hd4}} \\rightarrow20_{\\text{hd5}} \\rightarrow1_{\\text{ouput}}\\]\n\nThe activations argument holds the account of the activation function. It could be a string, a literal function, or a mix of it in a vector of inputs.\nThen, set activations = NULL, where NULL is the default value, which leads to set ReLU (nnf_relu) as the activation function for all hidden layers\n\nCodeif (is.null(activations)) {\n    activations = c(rep(\"nnf_relu\", length(hd_neurons)), NA)\n}\n\n\nEvery activations will have NA as the last element because we need to ensure no activation function after the output. The output layer often doesn’t need an activation (for regression) or needs a specific one based on the task (softmax for classification, sigmoid for binary classification). By defaulting to NA, the user can decide.\n\n\n\n\n\n\nLength of inputs\n\n\n\nTo provide values in activations argument, it needs to be equal to the size of hidden layers, or if you provide only 1 act. function, this will be the activation function across the transformations.\n\n\n\n\n\n\n\n\nDefault\n\n\n\nThe default is NULL. That is, if activations is not provided, the activation function is set to ReLU function.\n\n\n\n\n\n\n\n\nInstead of NULL\n\n\n\nNow, if you’re asking “Why needs to set activations to \"nnf_relu\" instead of NULL”? Don’t worry, I did consider that, but this is just a pure demo.\n\n\n\nThis part (re)processes the activation function inputs in the activations argument. This kept tracks the argument you are putting, especially when you the input you are writing in activations argument has different types.\n\nCodecall_args = match.call()\nactivation_arg = call_args$activations\n\nactivations = map2(activations, seq_along(activations), function(x, i) {\n    if (is.null(x)) {\n        NULL\n    } else if (is.function(x)) {\n        if(!is.null(activation_arg) && is.call(activation_arg) && \n           activation_arg[[1]] == quote(c)) {\n            func_name = as.character(activation_arg[[i + 1]])\n            sym(func_name)\n        } else {\n            func_name = names(which(sapply(ls(envir = parent.frame()), \n                function(name) {\n                    identical(get(name, envir = parent.frame()), x)\n                })))[1]\n            if (!is.na(func_name)) {\n                sym(func_name)\n            } else {\n                stop(\"Could not determine function name for activation function\")\n            }\n        }\n    } else if (is.character(x)) {\n        if (length(x) == 1 && is.na(x)) {\n            NULL\n        } else {\n            sym(x)\n        }\n    } else if (is.symbol(x)) {\n        x\n    } else if (is.logical(x) && length(x) == 1 && is.na(x)) {\n        NULL\n    } else {\n        stop(\"Activation must be a function, string, symbol, NA, or NULL\")\n    }\n})\n\n\n\nThe body I am referring in initialize method is the body of the function for the initialize implemented method. This part is a bit far from trivial. I named it init_body to keep track the expression I am trying to build.\n\n\n\n\n\n\nIn reality\n\n\n\nKeep in mind that there’s no initialize and forward parameters within nn_module() torch namespace or whatsoever. However, it is expected you to create them to create a module inside nn_module(). These parameters are kept within the ... wildcard parameter.\n\n\n\nHere is the part I am tracking inside create_nn_module body expression:\n\nCodeinit_body = map2(1:n_layers, map2(nodes[-length(nodes)], nodes[-1], c), \n    function(i, dims) {\n        layer_name = if (i == n_layers) \"out\" else glue(\"fc{i}\")\n        call2(\"=\", call2(\"$\", expr(self), sym(layer_name)), \n              call2(\"nn_linear\", !!!dims))\n    })\n\n\nWhat it does is it creates assignment expressions for each layer in the network.\nFor instance, c(20, 30, 20, 15, 20) is your argument for the activations:\n\n\nmap2(nodes[-length(nodes)], nodes[-1], c) pairs consecutive layer sizes:\n\nCodelist(\n    c(10, 20), \n    c(20, 30), \n    c(30, 20), \n    c(20, 15), \n    c(15, 20), \n    c(20, 1)\n)\n\n\n\n\nFor each pair, generates a layer assignment expression:\n\nLayer names: fc1, fc2, …, out (last layer)\nCreates: self$fc1 = nn_linear(10, 20)\n\n\n\n\nThis will be the generated expression:\nself$fc1 = nn_linear(10, 20)\nself$fc2 = nn_linear(20, 30)\nself$fc3 = nn_linear(30, 20)\nself$fc4 = nn_linear(20, 15)\nself$fc5 = nn_linear(15, 20)\nself$out = nn_linear(20, 1)\n\n\n\n\n\n\nHow is it done?\n\n\n\nI need you to understand rlang::call2() a bit:\nThe call2() function is a glorified call() from base R that builds function call expressions.\nFrom what I did within init_body:\n\ncall2(\"$\", expr(self), sym(\"fc1\")) constructs self$fc1\n\ncall2(\"nn_linear\", !!!dims) is a bit complex:\n\nIt splices dims from what I created in map2(nodes[-length(nodes)], nodes[-1], c).\n\ncall2() function accepts rlang’s quasiquotation API, then splices the dimensions, i.e. call2(\"nn_linear\", !!!c(10, 20)) to call2(\"nn_linear\", 10, 20).\nThen finally constructs nn_linear(10, 20)\n\n\n\ncall2(\"=\", lhs, rhs) parses an expression: lhs = rhs. This part yields an expression I want: self$fc1 = nn_linear(10, 20).\n\nNote: You can use &lt;- if you want, instead of =. After all, = within call2()’s .fn argument tokenize = as an assignment operator. \n\n\n\nNow, for this part:\n\nCodeinit = new_function(\n    args = list(), \n    body = call2(\"{\", !!!init_body)\n)\n\n\nDon’t forget to put curly brackets { around the built expression because it becomes necessary in R when composing a function with multiple lines. Still using call2() for that, particularly call2(\"{\", !!!init_body) creates a code block { ... } containing all initialization statements. The !!! operator “splices” the list of expressions into the block, because init_body forms a list of expressions.\nAfter building the expression I want for the body of initialize, let’s take further by utilizing it as a body to create a function with rlang::new_function. I just simply wraps all the layer initialization expressions into a complete function for initialize method for nn_module().\n\n\n\n\n\n\nInputs in initialize\n\n\n\nNotice that the argument for initialize is empty? I could’ve place input_size and output_size if I wanted to, but it seems unnecessary since I already placed the sizes of the input and output within the expression I built. To make a function expression with empty arguments, place the args argument of new_function with empty list().\n\n\nHere’s the result:\nfunction() {\n    self$fc1 = nn_linear(10, 20)\n    self$fc2 = nn_linear(20, 30)\n    self$fc3 = nn_linear(30, 20)\n    self$fc4 = nn_linear(20, 15)\n    self$fc5 = nn_linear(15, 20)\n    self$out = nn_linear(20, 1)\n}\nStore this expression into init because we still have to finalize the expression we want to create. \n\nThe same process as initialize, except we are not building multiple lines of expression, just building a chained expression with ‘magrittr’ pipe from the initial value.\n\nTo form this expression is also complex\n\nCodelayer_calls = map(1:n_layers, function(i) {\n    layer_name = if (i == n_layers) \"out\" else glue(\"fc{i}\")\n    activation_fn = if (i &lt;= length(activations)) activations[[i]] else NULL\n    \n    result = list(call2(call2(\"$\", expr(self), sym(layer_name))))\n    if (!is.null(activation_fn)) {\n        result = append(result, list(call2(activation_fn)))\n    }\n    result\n}) |&gt; \n    unlist() |&gt; \n    compact()\n\n\nWhat it does is it builds a sequence of operations for the forward pass: layer calls and their activation functions. I stored the output into layer_calls so that we can keep track of it.\nThe process:\n\n\nFor each layer, create a list containing:\n\nThe layer call: self$fc1()\n\nThe activation call (if exists): nnf_relu()\n\n\n\nFlatten all lists into a single sequence with unlist().\nFilter the list we created away from any NULL values with purrr::compact().\n\nThus, we form a list of expressions:\n\nCodelist(\n    self$fc1(), nnf_relu(),\n    self$fc2(), nnf_relu(),\n    self$fc3(), nnf_relu(),\n    self$fc4(), nnf_relu(),\n    self$fc5(), nnf_relu(),\n    self$out()\n)\n\n\nNote: The last layer (out) has no activation because we set it to NA.\n\nI choose to chain them, x or the input as the initial value, and choose not to break lines and forms multiple assignments. This is what I preferred, and besides, it’s so easy to form chained expression when the output is a defused call with reduce().\n\nCodeforward_body = reduce(layer_calls, function(acc, call) {\n    expr(!!acc %&gt;% !!call)\n}, .init = expr(x))\n\n\nI choose to chain all operations together with pipe operator %&gt;% from ‘magrittr’.\nThen, with reduce() works:\n\n\nStarting with x, it progressively adds each operation:\n\nStep 1: x %&gt;% self$fc1()\n\nStep 2: (x %&gt;% self$fc1()) %&gt;% nnf_relu()\n\nStep 3: (x %&gt;% self$fc1() %&gt;% nnf_relu()) %&gt;% self$fc2()\n\n…and so on\n\n\n\nAs for the final output:\nx %&gt;% self$fc1() %&gt;% nnf_relu() %&gt;% \n    self$fc2() %&gt;% nnf_relu() %&gt;% \n    self$fc3() %&gt;% nnf_relu() %&gt;% \n    self$fc4() %&gt;% nnf_relu() %&gt;% \n    self$fc5() %&gt;% nnf_relu() %&gt;% \n    self$out()\n\n\n\n\n\n\n\n\nWhy pipes?\n\n\n\nThe pipe operator makes the forward pass logic read like a natural sequence: “take input x, pass through fc1, apply nnf_relu to invoke ReLU activation function, then pass through fc2, apply nnf_relu to invoke ReLU activation function, …, it kepts repeating until we reach to out”\n\n\nAfter that, I stored it into forward_body, then make use of it to build the function for forward method with rlang::new_function():\n\nCodeforward = new_function(\n    args = list(x = expr()), \n    body = call2(\"{\", forward_body)\n)\n\n\nThe args for forward method has x with empty value. Then, wrap the piped forward pass into a function that accepts input x.\nAnd here’s the result:\nfunction(x) {\n    x %&gt;% self$fc1() %&gt;% nnf_relu() %&gt;% \n        self$fc2() %&gt;% nnf_relu() %&gt;% \n        self$fc3() %&gt;% nnf_relu() %&gt;% \n        self$fc4() %&gt;% nnf_relu() %&gt;% \n        self$fc5() %&gt;% nnf_relu() %&gt;% \n        self$out()\n}\nStore this expression into forward because we still have to finalize the expression we want to create. \n\nHere we are for the final part: generating the nn_module expression, by puzzling each part: initialize and forward.\nThe final part is built from this:\ncall2(\"nn_module\", nn_name, !!!set_names(list(init, forward), c(\"initialize\", \"forward\")))\nI mean, you still have to use call2() to build a call. The inputs should be:\n\n.fn = \"nn_module\" -&gt;\n\nThe rest of the arguments:\n\n\nnn_name which is equivalent to “DeepFFN”. You can set any names whatever you want, though.\ninitialize = init\nforward = forward\nOriginally, I formed this in this expression: !!!set_names(list(init, forward), c(\"initialize\", \"forward\")). But then, I realized that we only need initialize and forward, and besides, this is a bit overkill.\n\n\n\nThus, the final expression that defines the neural network module.\nAnd hence, I form a function that generates a, perhaps, template:\n\nhd_nodes = c(20, 30, 20, 15, 20)\nact_fns = c(\"nnf_relu\", \"nnf_relu\", \"nnf_relu\", \"nnf_relu\")\ncreate_nn_module(\n    hd_neurons = hd_nodes, \n    activations = act_fns\n)\n\nnn_module(\"DeepFFN\", initialize = function () \n{\n    self$fc1 = nn_linear(10, 20)\n    self$fc2 = nn_linear(20, 30)\n    self$fc3 = nn_linear(30, 20)\n    self$fc4 = nn_linear(20, 15)\n    self$fc5 = nn_linear(15, 20)\n    self$out = nn_linear(20, 1)\n}, forward = function (x) \n{\n    x %&gt;% self$fc1() %&gt;% nnf_relu() %&gt;% self$fc2() %&gt;% nnf_relu() %&gt;% \n        self$fc3() %&gt;% nnf_relu() %&gt;% self$fc4() %&gt;% nnf_relu() %&gt;% \n        self$fc5() %&gt;% self$out()\n})\n\n\n\nThis is an advanced example of metaprogramming in R, demonstrating how to leverage functional programming and rlang for code generation. I don’t mind you to replicate what I did, but sometimes this technique should be used judiciously—sometimes simpler, more explicit code is better.\nThis example showcases:\n\nDeep understanding of R’s evaluation model\nFunctional programming with purrr\n\nExpression manipulation with rlang\n\nPractical application to deep learning workflows\n\nAnd also, I am aware to the fact that the function I made is ugly if you said so."
  },
  {
    "objectID": "posts/01-meta-nn/index.html#introduction",
    "href": "posts/01-meta-nn/index.html#introduction",
    "title": "Automatically generate Deep Feedforward Neural Network (DFFNN) module from torch expression",
    "section": "",
    "text": "The create_nn_module() function dynamically generates torch neural network module definitions. Instead of manually writing out layer definitions and forward pass logic, this function builds the code expressions for you.\nKey benefits:\n\n\nFlexibility: Change network architecture with a single function call\n\nAutomation: Generate multiple network configurations programmatically\n\nExperimentation: Quickly test different architectures in hyperparameter searches\n\nThis is how it’s done:\n\nDefine the network architecture (input size, hidden layers, output size)\nSpecify activation functions for each layer\nProgrammatically generate the initialize method (layer definitions)\nProgrammatically generate the forward method (forward pass logic)\nReturn an nn_module expression ready to be evaluated\n\nThe packages used:\n\n\nrlang (v1.1.4) - For metaprogramming tools\n\npurrr (v1.0.2) - For functional programming\n\nglue (v1.7.0) - For string interpolation\n\nmagrittr - For pipe operator\n\nbox (v1.2.0) - For modular code organization"
  },
  {
    "objectID": "posts/01-meta-nn/index.html#the-complete-function",
    "href": "posts/01-meta-nn/index.html#the-complete-function",
    "title": "Automatically generate Deep Feedforward Neural Network (DFFNN) module from torch expression",
    "section": "",
    "text": "I created create_nn_module() function a while ago and shared it on GitHub Gist. Here’s the function we’ll be analyzing:\n\nCodecreate_nn_module = function(nn_name = \"DeepFFN\", hd_neurons = c(20, 30, 20, 15), no_x = 10, no_y = 1, activations = NULL) {\n    box::use(\n        rlang[new_function, call2, caller_env, expr, exprs, sym, is_function, env_get_list],\n        purrr[map, map2, reduce, set_names, compact, map_if, keep, map_lgl], \n        glue[glue], \n        magrittr[`%&gt;%`]\n    )\n    \n    nodes = c(no_x, hd_neurons, no_y)\n    n_layers = length(nodes) - 1\n    \n    call_args = match.call()\n    activation_arg = call_args$activations\n    \n    if (is.null(activations)) {\n        activations = c(rep(\"nnf_relu\", length(hd_neurons)), NA)\n    } else if (length(activations) == 1 || is.function(activations)) {\n        single_activation = activations\n        activations = c(rep(list(single_activation), length(hd_neurons)), list(NA))\n    }\n    \n    activations = map2(activations, seq_along(activations), function(x, i) {\n        if (is.null(x)) {\n            NULL\n        } else if (is.function(x)) {\n            if(!is.null(activation_arg) && is.call(activation_arg) && activation_arg[[1]] == quote(c)) {\n                func_name = as.character(activation_arg[[i + 1]])\n                sym(func_name)\n            } else if(!is.null(activation_arg) && (is.symbol(activation_arg) || is.character(activation_arg))) {\n                func_name = as.character(activation_arg)\n                sym(func_name)\n            } else {\n                parent_env = parent.frame()\n                env_names = ls(envir = parent_env)\n                matching_names = env_names %&gt;%\n                    keep(~ {\n                        obj = env_get_list(parent_env, .x)[[1]]\n                        identical(obj, x)\n                    })\n                \n                if (length(matching_names) &gt; 0) {\n                    sym(matching_names[1])\n                } else {\n                    stop(\"Could not determine function name for activation function\")\n                }\n            }\n        } else if (is.character(x)) {\n            if (length(x) == 1 && is.na(x)) {\n                NULL\n            } else {\n                sym(x)\n            }\n        } else if (is.symbol(x)) {\n            x\n        } else if (is.logical(x) && length(x) == 1 && is.na(x)) {\n            NULL\n        } else {\n            stop(\"Activation must be a function, string, symbol, NA, or NULL\")\n        }\n    })\n    \n    init_body = map2(1:n_layers, map2(nodes[-length(nodes)], nodes[-1], c), function(i, dims) {\n        layer_name = if (i == n_layers) \"out\" else glue(\"fc{i}\")\n        call2(\"=\", call2(\"$\", expr(self), sym(layer_name)), call2(\"nn_linear\", !!!dims))\n    })\n    \n    init = new_function(\n        args = list(), \n        body = call2(\"{\", !!!init_body)\n    )\n    \n    layer_calls = map(1:n_layers, function(i) {\n        layer_name = if (i == n_layers) \"out\" else glue(\"fc{i}\")\n        activation_fn = if (i &lt;= length(activations)) activations[[i]] else NULL\n        \n        result = list(call2(call2(\"$\", expr(self), sym(layer_name))))\n        if (!is.null(activation_fn)) {\n            result = append(result, list(call2(activation_fn)))\n        }\n        result\n    }) |&gt; \n        unlist() |&gt; # recursive = FALSE is also valid\n        compact()\n    \n    forward_body = reduce(layer_calls, function(acc, call) {\n        expr(!!acc %&gt;% !!call)\n    }, .init = expr(x))\n    \n    forward = new_function(\n        args = list(x = expr()), \n        body = call2(\"{\", forward_body)\n    )\n    \n    call2(\"nn_module\", nn_name, initialize = init, forward = forward)\n}\n\n\n\n\nYou’ll notice that I’ve been using another approach to load namespace in R. But, why ‘box’? You need to check out my mini book dedicated on modular programming in R.\n\nWell, a function, or a function call, creates an environment, which encloses the objects and operations within it. In other words, we create a closure. This is actually a good practice for several reasons:\n\nNamespace isolation: Dependencies loaded inside the function will not make pollution the global environment, or conflicts with any packages loaded. When you load packages required with library(), inside a function or not, it attaches those packages to your search path, and will mask functions from other packages. With box::use() inside a function, the imports are scoped only to that function’s or call’s environment.\nExplicit dependencies: Anyone reading the function immediately knows what external tools it uses. You don’t have to scroll to the top of a script to see what’s loaded.\nReproducibility: The function becomes self-contained. If you share just this function, others know exactly what packages they need less hunting through documentation.\nAvoiding conflicts: Different functions can use different versions or implementations without conflicts. For example, one function might use dplyr::filter() while another uses stats::filter(), and they won’t interfere with each other.\nLazy loading: The packages are only loaded when the function is actually called, not when it’s defined. This can improve script startup time if you have many functions but only use a few.\n\n\n\n\n\n\n\nNote\n\n\n\nIn a nutshell: The ‘box’ package provides explicit, granular imports, making it transparent which namespace to be imported from which packages. It’s like having a well-organized toolbox where each tool is labeled."
  },
  {
    "objectID": "posts/01-meta-nn/index.html#explanations",
    "href": "posts/01-meta-nn/index.html#explanations",
    "title": "Automatically generate Deep Feedforward Neural Network (DFFNN) module from torch expression",
    "section": "",
    "text": "I’ll be trying to be concise on explaining each layers of the function so that you’ll understand what I did\n\nI use box::use() to load dependencies:\n\n\nrlang: Improvised Core R programming. One of the core R programming, metaprogramming which includes creating expressions and functions programmatically, are less painful than what base R have.\n\npurrr: Improvised Functional programming utilities.\n\nglue: R lacks Python’s f-string for string interpolation, although we have sprintf() and paste() for that. glue makes string interpolation more readable with glue(\"fc{i}\") instead of paste0(\"fc\", i) or sprintf(\"fc%d\", i).\n\nmagrittr: The pipe operator %&gt;% for chaining operations. This is optional, by the way — R 4.1+ has the native pipe |&gt;, but %&gt;% offers better flexibility with the dot placeholder.\n\nIn DFFNN architecture, it is defined by the input layer, the hidden layer, and the output layer.\n\n\nSource: https://medium.com/data-science/designing-your-neural-networks-a5e4617027ed\n\nThe number of nodes are defined by integers, except for input and output layer nodes which they are fixed and determined by the data you provided, and they are defined by no_x and no_y. The number of hidden layers is defined by the length of input in hd_neurons argument.\nCombine no_x, hd_neurons, no_y in order:\nnodes = c(no_x, hd_neurons, no_y)\nAnd then calculate the length of nodes, which is \\(1 + n_{\\text{hidden layres}} + 1\\), and then subtract it by 1 because the applied activation functions is invoked between each layer.\nn_layers = length(nodes) - 1\n\nWhen you have:\n\n10 predictors\n\n5 hidden layers, and for each layer:\n\n20 nodes\n30 nodes\n20 nodes\n15 nodes\n20 nodes\n\n\n1 response variable\n\nTotal number of layers: 7\nThis means we need 7 - 1 linear transformations, and here is my diagram:\n\\[10_{\\text{inputs}} \\rightarrow20_{\\text{hd1}} \\rightarrow30_{\\text{hd2}} \\rightarrow20_{\\text{hd3}} \\rightarrow15_{\\text{hd4}} \\rightarrow20_{\\text{hd5}} \\rightarrow1_{\\text{ouput}}\\]\n\nThe activations argument holds the account of the activation function. It could be a string, a literal function, or a mix of it in a vector of inputs.\nThen, set activations = NULL, where NULL is the default value, which leads to set ReLU (nnf_relu) as the activation function for all hidden layers\n\nCodeif (is.null(activations)) {\n    activations = c(rep(\"nnf_relu\", length(hd_neurons)), NA)\n}\n\n\nEvery activations will have NA as the last element because we need to ensure no activation function after the output. The output layer often doesn’t need an activation (for regression) or needs a specific one based on the task (softmax for classification, sigmoid for binary classification). By defaulting to NA, the user can decide.\n\n\n\n\n\n\nLength of inputs\n\n\n\nTo provide values in activations argument, it needs to be equal to the size of hidden layers, or if you provide only 1 act. function, this will be the activation function across the transformations.\n\n\n\n\n\n\n\n\nDefault\n\n\n\nThe default is NULL. That is, if activations is not provided, the activation function is set to ReLU function.\n\n\n\n\n\n\n\n\nInstead of NULL\n\n\n\nNow, if you’re asking “Why needs to set activations to \"nnf_relu\" instead of NULL”? Don’t worry, I did consider that, but this is just a pure demo.\n\n\n\nThis part (re)processes the activation function inputs in the activations argument. This kept tracks the argument you are putting, especially when you the input you are writing in activations argument has different types.\n\nCodecall_args = match.call()\nactivation_arg = call_args$activations\n\nactivations = map2(activations, seq_along(activations), function(x, i) {\n    if (is.null(x)) {\n        NULL\n    } else if (is.function(x)) {\n        if(!is.null(activation_arg) && is.call(activation_arg) && \n           activation_arg[[1]] == quote(c)) {\n            func_name = as.character(activation_arg[[i + 1]])\n            sym(func_name)\n        } else {\n            func_name = names(which(sapply(ls(envir = parent.frame()), \n                function(name) {\n                    identical(get(name, envir = parent.frame()), x)\n                })))[1]\n            if (!is.na(func_name)) {\n                sym(func_name)\n            } else {\n                stop(\"Could not determine function name for activation function\")\n            }\n        }\n    } else if (is.character(x)) {\n        if (length(x) == 1 && is.na(x)) {\n            NULL\n        } else {\n            sym(x)\n        }\n    } else if (is.symbol(x)) {\n        x\n    } else if (is.logical(x) && length(x) == 1 && is.na(x)) {\n        NULL\n    } else {\n        stop(\"Activation must be a function, string, symbol, NA, or NULL\")\n    }\n})\n\n\n\nThe body I am referring in initialize method is the body of the function for the initialize implemented method. This part is a bit far from trivial. I named it init_body to keep track the expression I am trying to build.\n\n\n\n\n\n\nIn reality\n\n\n\nKeep in mind that there’s no initialize and forward parameters within nn_module() torch namespace or whatsoever. However, it is expected you to create them to create a module inside nn_module(). These parameters are kept within the ... wildcard parameter.\n\n\n\nHere is the part I am tracking inside create_nn_module body expression:\n\nCodeinit_body = map2(1:n_layers, map2(nodes[-length(nodes)], nodes[-1], c), \n    function(i, dims) {\n        layer_name = if (i == n_layers) \"out\" else glue(\"fc{i}\")\n        call2(\"=\", call2(\"$\", expr(self), sym(layer_name)), \n              call2(\"nn_linear\", !!!dims))\n    })\n\n\nWhat it does is it creates assignment expressions for each layer in the network.\nFor instance, c(20, 30, 20, 15, 20) is your argument for the activations:\n\n\nmap2(nodes[-length(nodes)], nodes[-1], c) pairs consecutive layer sizes:\n\nCodelist(\n    c(10, 20), \n    c(20, 30), \n    c(30, 20), \n    c(20, 15), \n    c(15, 20), \n    c(20, 1)\n)\n\n\n\n\nFor each pair, generates a layer assignment expression:\n\nLayer names: fc1, fc2, …, out (last layer)\nCreates: self$fc1 = nn_linear(10, 20)\n\n\n\n\nThis will be the generated expression:\nself$fc1 = nn_linear(10, 20)\nself$fc2 = nn_linear(20, 30)\nself$fc3 = nn_linear(30, 20)\nself$fc4 = nn_linear(20, 15)\nself$fc5 = nn_linear(15, 20)\nself$out = nn_linear(20, 1)\n\n\n\n\n\n\nHow is it done?\n\n\n\nI need you to understand rlang::call2() a bit:\nThe call2() function is a glorified call() from base R that builds function call expressions.\nFrom what I did within init_body:\n\ncall2(\"$\", expr(self), sym(\"fc1\")) constructs self$fc1\n\ncall2(\"nn_linear\", !!!dims) is a bit complex:\n\nIt splices dims from what I created in map2(nodes[-length(nodes)], nodes[-1], c).\n\ncall2() function accepts rlang’s quasiquotation API, then splices the dimensions, i.e. call2(\"nn_linear\", !!!c(10, 20)) to call2(\"nn_linear\", 10, 20).\nThen finally constructs nn_linear(10, 20)\n\n\n\ncall2(\"=\", lhs, rhs) parses an expression: lhs = rhs. This part yields an expression I want: self$fc1 = nn_linear(10, 20).\n\nNote: You can use &lt;- if you want, instead of =. After all, = within call2()’s .fn argument tokenize = as an assignment operator. \n\n\n\nNow, for this part:\n\nCodeinit = new_function(\n    args = list(), \n    body = call2(\"{\", !!!init_body)\n)\n\n\nDon’t forget to put curly brackets { around the built expression because it becomes necessary in R when composing a function with multiple lines. Still using call2() for that, particularly call2(\"{\", !!!init_body) creates a code block { ... } containing all initialization statements. The !!! operator “splices” the list of expressions into the block, because init_body forms a list of expressions.\nAfter building the expression I want for the body of initialize, let’s take further by utilizing it as a body to create a function with rlang::new_function. I just simply wraps all the layer initialization expressions into a complete function for initialize method for nn_module().\n\n\n\n\n\n\nInputs in initialize\n\n\n\nNotice that the argument for initialize is empty? I could’ve place input_size and output_size if I wanted to, but it seems unnecessary since I already placed the sizes of the input and output within the expression I built. To make a function expression with empty arguments, place the args argument of new_function with empty list().\n\n\nHere’s the result:\nfunction() {\n    self$fc1 = nn_linear(10, 20)\n    self$fc2 = nn_linear(20, 30)\n    self$fc3 = nn_linear(30, 20)\n    self$fc4 = nn_linear(20, 15)\n    self$fc5 = nn_linear(15, 20)\n    self$out = nn_linear(20, 1)\n}\nStore this expression into init because we still have to finalize the expression we want to create. \n\nThe same process as initialize, except we are not building multiple lines of expression, just building a chained expression with ‘magrittr’ pipe from the initial value.\n\nTo form this expression is also complex\n\nCodelayer_calls = map(1:n_layers, function(i) {\n    layer_name = if (i == n_layers) \"out\" else glue(\"fc{i}\")\n    activation_fn = if (i &lt;= length(activations)) activations[[i]] else NULL\n    \n    result = list(call2(call2(\"$\", expr(self), sym(layer_name))))\n    if (!is.null(activation_fn)) {\n        result = append(result, list(call2(activation_fn)))\n    }\n    result\n}) |&gt; \n    unlist() |&gt; \n    compact()\n\n\nWhat it does is it builds a sequence of operations for the forward pass: layer calls and their activation functions. I stored the output into layer_calls so that we can keep track of it.\nThe process:\n\n\nFor each layer, create a list containing:\n\nThe layer call: self$fc1()\n\nThe activation call (if exists): nnf_relu()\n\n\n\nFlatten all lists into a single sequence with unlist().\nFilter the list we created away from any NULL values with purrr::compact().\n\nThus, we form a list of expressions:\n\nCodelist(\n    self$fc1(), nnf_relu(),\n    self$fc2(), nnf_relu(),\n    self$fc3(), nnf_relu(),\n    self$fc4(), nnf_relu(),\n    self$fc5(), nnf_relu(),\n    self$out()\n)\n\n\nNote: The last layer (out) has no activation because we set it to NA.\n\nI choose to chain them, x or the input as the initial value, and choose not to break lines and forms multiple assignments. This is what I preferred, and besides, it’s so easy to form chained expression when the output is a defused call with reduce().\n\nCodeforward_body = reduce(layer_calls, function(acc, call) {\n    expr(!!acc %&gt;% !!call)\n}, .init = expr(x))\n\n\nI choose to chain all operations together with pipe operator %&gt;% from ‘magrittr’.\nThen, with reduce() works:\n\n\nStarting with x, it progressively adds each operation:\n\nStep 1: x %&gt;% self$fc1()\n\nStep 2: (x %&gt;% self$fc1()) %&gt;% nnf_relu()\n\nStep 3: (x %&gt;% self$fc1() %&gt;% nnf_relu()) %&gt;% self$fc2()\n\n…and so on\n\n\n\nAs for the final output:\nx %&gt;% self$fc1() %&gt;% nnf_relu() %&gt;% \n    self$fc2() %&gt;% nnf_relu() %&gt;% \n    self$fc3() %&gt;% nnf_relu() %&gt;% \n    self$fc4() %&gt;% nnf_relu() %&gt;% \n    self$fc5() %&gt;% nnf_relu() %&gt;% \n    self$out()\n\n\n\n\n\n\n\n\nWhy pipes?\n\n\n\nThe pipe operator makes the forward pass logic read like a natural sequence: “take input x, pass through fc1, apply nnf_relu to invoke ReLU activation function, then pass through fc2, apply nnf_relu to invoke ReLU activation function, …, it kepts repeating until we reach to out”\n\n\nAfter that, I stored it into forward_body, then make use of it to build the function for forward method with rlang::new_function():\n\nCodeforward = new_function(\n    args = list(x = expr()), \n    body = call2(\"{\", forward_body)\n)\n\n\nThe args for forward method has x with empty value. Then, wrap the piped forward pass into a function that accepts input x.\nAnd here’s the result:\nfunction(x) {\n    x %&gt;% self$fc1() %&gt;% nnf_relu() %&gt;% \n        self$fc2() %&gt;% nnf_relu() %&gt;% \n        self$fc3() %&gt;% nnf_relu() %&gt;% \n        self$fc4() %&gt;% nnf_relu() %&gt;% \n        self$fc5() %&gt;% nnf_relu() %&gt;% \n        self$out()\n}\nStore this expression into forward because we still have to finalize the expression we want to create. \n\nHere we are for the final part: generating the nn_module expression, by puzzling each part: initialize and forward.\nThe final part is built from this:\ncall2(\"nn_module\", nn_name, !!!set_names(list(init, forward), c(\"initialize\", \"forward\")))\nI mean, you still have to use call2() to build a call. The inputs should be:\n\n.fn = \"nn_module\" -&gt;\n\nThe rest of the arguments:\n\n\nnn_name which is equivalent to “DeepFFN”. You can set any names whatever you want, though.\ninitialize = init\nforward = forward\nOriginally, I formed this in this expression: !!!set_names(list(init, forward), c(\"initialize\", \"forward\")). But then, I realized that we only need initialize and forward, and besides, this is a bit overkill.\n\n\n\nThus, the final expression that defines the neural network module.\nAnd hence, I form a function that generates a, perhaps, template:\n\nhd_nodes = c(20, 30, 20, 15, 20)\nact_fns = c(\"nnf_relu\", \"nnf_relu\", \"nnf_relu\", \"nnf_relu\")\ncreate_nn_module(\n    hd_neurons = hd_nodes, \n    activations = act_fns\n)\n\nnn_module(\"DeepFFN\", initialize = function () \n{\n    self$fc1 = nn_linear(10, 20)\n    self$fc2 = nn_linear(20, 30)\n    self$fc3 = nn_linear(30, 20)\n    self$fc4 = nn_linear(20, 15)\n    self$fc5 = nn_linear(15, 20)\n    self$out = nn_linear(20, 1)\n}, forward = function (x) \n{\n    x %&gt;% self$fc1() %&gt;% nnf_relu() %&gt;% self$fc2() %&gt;% nnf_relu() %&gt;% \n        self$fc3() %&gt;% nnf_relu() %&gt;% self$fc4() %&gt;% nnf_relu() %&gt;% \n        self$fc5() %&gt;% self$out()\n})"
  },
  {
    "objectID": "posts/01-meta-nn/index.html#disclaimer",
    "href": "posts/01-meta-nn/index.html#disclaimer",
    "title": "Automatically generate Deep Feedforward Neural Network (DFFNN) module from torch expression",
    "section": "",
    "text": "This is an advanced example of metaprogramming in R, demonstrating how to leverage functional programming and rlang for code generation. I don’t mind you to replicate what I did, but sometimes this technique should be used judiciously—sometimes simpler, more explicit code is better.\nThis example showcases:\n\nDeep understanding of R’s evaluation model\nFunctional programming with purrr\n\nExpression manipulation with rlang\n\nPractical application to deep learning workflows\n\nAnd also, I am aware to the fact that the function I made is ugly if you said so."
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Check out my blogs",
    "section": "",
    "text": "Get notified when I publish new posts. No spam, unsubscribe anytime"
  },
  {
    "objectID": "posts/index.html#subscribe-to-my-newsletter",
    "href": "posts/index.html#subscribe-to-my-newsletter",
    "title": "Check out my blogs",
    "section": "",
    "text": "Get notified when I publish new posts. No spam, unsubscribe anytime"
  },
  {
    "objectID": "posts/03-modules-in-r/index.html",
    "href": "posts/03-modules-in-r/index.html",
    "title": "Box: Placing module system into R",
    "section": "",
    "text": "Click here to read the actual book. \nWelcome to the definitive guide to module system in R using box!\nWhether you want another way to import R packages, or simply seeking better ways to organize your code, this guide helps you to reimagine something did not exist in R for quite a long time: module system. This book introduces you the package that I discovered few years ago, it is called box, initially modules package, by Konrad Rudolph. This package provides a clean, straightforward API to define and manage modules. R, especially its users, suffers from managing huge R pipelines, because there’s no clean way to manage such complex pipelines…this problem will potentially cease to exists, as box package affords closely equivalent module system to other programming languages like Python, C#, and more.\n\n\nA definitive guide that walks you through:\n\nAlternative approach to import R codes (i.e. R packages, modules)\nModern approaches to compose and organize R codes\nStep-by-step tutorials for using the box package\nBest practices for maintainable R code\n\n\n\n\nOnly little to no books teach you to correctly write reusable, composable, and modular R codes :). Most of the books maybe teaches you about R, particularly application of R in different fields, but little to none explains one of the best practices. Most of them uses library() anyways, so you won’t certainly find similar book like this.\nConsequently, while R offers various ways to organize code, the box package manages to be superior among them (I am bias) by introducing a fresh, modern approach to module system that may significantly improve your R development workflow, similar to the workflow made by other developers, particularly Python devs. This book bridges the gap between basic R programming and professional-grade code organization.\n\n\n\nClick here and it will send you to the actual book. \nThe content is structured progressively, building from foundational concepts to deep applications. For the best learning experience:\n\nStart with the introduction to understand the core concepts\nLearning the fundamentals of import system with {box} package\nLearning the structures and constructions of reusability of modules, treating them like R packages, and supplied with documentation\nLearning unit testing the modules\n\n\n\n\nContributions are welcome! If you have suggestions or improvements, please open an issue or submit a pull request on the GitHub repository.\n\n\n\nThis book is licensed under the Creative Commons Attribution 4.0 International License (CC BY 4.0). See the LICENSE file for details.\n\n\n\nThis book was created by Joshua Marie.\nSpecial thanks to:\n\nKonrad Rudolph for creating and maintaining the box package\nThe R community for their continued support and feedback\n\nThe book is built with Quarto, hosted on GitHub Pages."
  },
  {
    "objectID": "posts/03-modules-in-r/index.html#what-youll-find-here",
    "href": "posts/03-modules-in-r/index.html#what-youll-find-here",
    "title": "Box: Placing module system into R",
    "section": "",
    "text": "A definitive guide that walks you through:\n\nAlternative approach to import R codes (i.e. R packages, modules)\nModern approaches to compose and organize R codes\nStep-by-step tutorials for using the box package\nBest practices for maintainable R code"
  },
  {
    "objectID": "posts/03-modules-in-r/index.html#why-this-book",
    "href": "posts/03-modules-in-r/index.html#why-this-book",
    "title": "Box: Placing module system into R",
    "section": "",
    "text": "Only little to no books teach you to correctly write reusable, composable, and modular R codes :). Most of the books maybe teaches you about R, particularly application of R in different fields, but little to none explains one of the best practices. Most of them uses library() anyways, so you won’t certainly find similar book like this.\nConsequently, while R offers various ways to organize code, the box package manages to be superior among them (I am bias) by introducing a fresh, modern approach to module system that may significantly improve your R development workflow, similar to the workflow made by other developers, particularly Python devs. This book bridges the gap between basic R programming and professional-grade code organization."
  },
  {
    "objectID": "posts/03-modules-in-r/index.html#how-to-use-this-guide",
    "href": "posts/03-modules-in-r/index.html#how-to-use-this-guide",
    "title": "Box: Placing module system into R",
    "section": "",
    "text": "Click here and it will send you to the actual book. \nThe content is structured progressively, building from foundational concepts to deep applications. For the best learning experience:\n\nStart with the introduction to understand the core concepts\nLearning the fundamentals of import system with {box} package\nLearning the structures and constructions of reusability of modules, treating them like R packages, and supplied with documentation\nLearning unit testing the modules"
  },
  {
    "objectID": "posts/03-modules-in-r/index.html#contributing",
    "href": "posts/03-modules-in-r/index.html#contributing",
    "title": "Box: Placing module system into R",
    "section": "",
    "text": "Contributions are welcome! If you have suggestions or improvements, please open an issue or submit a pull request on the GitHub repository."
  },
  {
    "objectID": "posts/03-modules-in-r/index.html#license",
    "href": "posts/03-modules-in-r/index.html#license",
    "title": "Box: Placing module system into R",
    "section": "",
    "text": "This book is licensed under the Creative Commons Attribution 4.0 International License (CC BY 4.0). See the LICENSE file for details."
  },
  {
    "objectID": "posts/03-modules-in-r/index.html#credits",
    "href": "posts/03-modules-in-r/index.html#credits",
    "title": "Box: Placing module system into R",
    "section": "",
    "text": "This book was created by Joshua Marie.\nSpecial thanks to:\n\nKonrad Rudolph for creating and maintaining the box package\nThe R community for their continued support and feedback\n\nThe book is built with Quarto, hosted on GitHub Pages."
  },
  {
    "objectID": "posts/04-tidyselect-helpers/index.html",
    "href": "posts/04-tidyselect-helpers/index.html",
    "title": "The Hidden Magic of Tidy-Select: R’s Universal Column Selection Language",
    "section": "",
    "text": "1 Introduction\nHave you ever wondered how where(), starts_with(), and other selection helpers work seamlessly across different tidyverse packages? I recently discovered something surprising: you can actually use these functions in dplyr, tidyr, and other packages that invokes &lt;tidy-select&gt; API, without explicitly loading them.\nHere’s how it works:\n\niris |&gt; \n    tidyr::pivot_longer(\n        cols = where(is.numeric), # using `where()` w/out calling dplyr / tidyselect\n        names_to = 'Variable',\n        values_to = 'Measure'\n    )\n\n\n  \n\n\n\nTake note that I never load tidyselect and dplyr (the where() function in dplyr is just one of many re-exports). Yet, where() works perfectly. It doesn’t belong to / re-exported by tidyr, but you can use where(), if and only if the functions is invoking &lt;tidy-select&gt; API.\n\n2 What Are These Functions Called?\nThese are officially called tidyselect helpers (or “selection language”). They’re part of the tidyselect package, which provides a domain-specific language (DSL) for selecting columns in data frames.\nYou might also hear them referred to as:\n\nSelection helper functions\n\n&lt;Tidy-select&gt; helpers\nColumn selection helpers\n\n3 The Complete Family of Selection Helpers\nThe tidyselect package can be divided into 3 categories of helpers.\n\n\nPattern Matching Helpers\nPredicate-Based Helpers\n“Positional” Helpers\n\n\n\n\n\nColumns starting with a prefix\n\niris |&gt; \n    dplyr::select(starts_with(\"Sepal\")) |&gt; \n    head(3)\n\n\n  \n\n\n\n\n\nColumns ending with a suffix\n\niris |&gt; \n    dplyr::select(ends_with(\"Width\")) |&gt; \n    head(3)\n\n\n  \n\n\n\n\n\nColumns containing a literal string\n\niris |&gt; \n    dplyr::select(contains(\"al\")) |&gt; \n    head(3)\n\n\n  \n\n\n\n\n\nColumns matching a regular expression\n\niris |&gt; \n    dplyr::select(matches(\"^Sepal\")) |&gt; \n    head(3)\n\n\n  \n\n\n\n\n\nColumns following the number pattern\n\niris |&gt; \n    dplyr::select(num_range('x', 1:4)) |&gt; \n    head(3)\n\n\n  \n\n\n\n\n\n\n\nThe where() function is similar to SQL WHERE, except it is functional that (should) returns a Boolean value that satisfies the condition.\n\niris |&gt; \n    dplyr::select(where(is.numeric)) |&gt; \n    head(3)\n\n\n  \n\n\niris |&gt; \n    dplyr::select(where(is.factor)) |&gt; \n    head(3)\n\n\n  \n\n\niris |&gt; \n    dplyr::select(where(\\(col) is.numeric(col) && mean(col) &gt; 3.5))\n\n\n  \n\n\n\n\n\nThese are functions that select columns based on their position in the data frame\n\n\neverything()\n\niris |&gt; \n    dplyr::select(everything()) |&gt; \n    head(3)\n\n\n  \n\n\n\nThis is equivalent to relocate(iris, Species):\n\niris |&gt; \n    dplyr::select(Species, everything()) |&gt; \n    head(3)\n\n\n  \n\n\n\n\nlast_col()\n\n\niris |&gt; \n    dplyr::select(last_col()) |&gt; \n    head(3)\n\n\n  \n\n\n\nOffset: 2nd to the last\n\niris |&gt; \n    dplyr::select(last_col(1)) |&gt; \n    head(3)\n\n\n  \n\n\n\nOffset: Multiple columns from the end\n\niris |&gt; \n    dplyr::select(last_col(2):last_col()) |&gt; \n    head(3)\n\n\n  \n\n\n\n\n\n\nNoticed that I invoked most of &lt;tidy-select&gt; helpers, but never loaded dplyr or tidyselect, not once, just to use them.\n\n4 “Data-Masking” Subset\nJust like those &lt;tidy-select&gt; helpers, some functions found in dplyr, but doesn’t in tidyselect. These are all the functions that can be used within “data-masking” functions, such as dplyr::mutate() and dplyr::summarise(). Take a note of the term “within”, which means, you can’t use them outside from “data-masking” functions.\nI call across(), if_any(), and if_all() as projection helpers because they correspond to the SELECT clause in SQL, except they both if_any(), and if_all() map over the selected columns and returns the Boolean vector, while the across() function modifies the selected columns. The pick() function, on the other hand serves as a complement of across() by extracting them as a data frame, however, this only applies to subset a data frame to be invoked within the operations in “data-masking” functions. All of them can make use of the &lt;tidy-select&gt; API, meaning you can apply selectors like starts_with() or everything() to specify which columns to project.\n\n\nUsing pick()\nUsing across()\nUsing if_all() / if_any()\nFunctions you actually need to attach\n\n\n\nHere’s an example: Calculating mean and standard deviation\n\niris |&gt; \n    dplyr::group_by(Species) |&gt; \n    dplyr::summarise(\n        summary = list({\n            num = pick(where(is.numeric))\n            tibble::tibble(\n                vars = colnames(num), \n                mean = colMeans(num),\n                sd = apply(num, 2, sd)\n            )\n        })\n    ) |&gt; \n    tidyr::unnest(summary)\n\n\n  \n\n\n\n\nI am aware there’s a better approach to calculate the mean and standard deviation of each column by group.\n\n\n\nHere’s an example: Apply min-max normalization among numeric columns in iris dataset\n\niris |&gt; \n    dplyr::as_tibble() |&gt; \n    dplyr::mutate(\n        across(\n            where(is.numeric), \n            \\(col) { col - min(col) } / { max(col) - min(col) }\n        )\n    )\n\n\n  \n\n\n\nAnd once again, I never attach dplyr into the search path just to use across() and pick().\nYou can use across() in some dplyr “data-masking” function like filter(), but this is a deprecated behavior and attaching dplyr package is required.\n\n\nExample: Removing all missing values across all columns in airquality data frame\n\nairquality |&gt; \n    dplyr::filter(if_all(everything(), \\(col) !is.na(col))) |&gt; \n    head(5)\n\n\n  \n\n\n\nIf if_all() / if_any() is used outside filter(), those functions need dplyr package to be attached to use them.\n\n\nThough, there are some exceptions: there are helper functions you actually need dplyr to be attached to use them, otherwise they don’t work and R will throw an error.\nHere they are:\n\nlibrary(dplyr)\n\n\n\nn()\n\niris |&gt;\n    group_by(Species) |&gt; \n    slice_max(n = 20, order_by = Sepal.Length) |&gt; \n    summarise(\n        count = n(), # 👈 \n        m_sl = mean(Sepal.Length)\n    )\n\n\n  \n\n\n\n\n\ncur_group()\n\nmtcars |&gt;\n    group_by(cyl) |&gt;\n    reframe({\n        model = lm(mpg ~ wt, data = cur_group()) # 👈 \n        coefs = coef(model)\n\n        tibble(\n            terms = names(coefs), \n            estimate = coefs\n        )\n    })\n\n\n  \n\n\n\n\n\ncur_group_id()\n\nstarwars |&gt;\n    group_by(species) |&gt;\n    reframe(\n        species, \n        name, \n        hierarchical_id = sprintf(\"%02d-%03d\", cur_group_id(), row_number()) # 👈 \n    ) |&gt; \n    slice_min(hierarchical_id, n = 15)\n\n\n  \n\n\n\n\n\ncur_group_rows()\n\niris |&gt; \n    group_by(Species) |&gt; \n    slice_sample(\n        n = 75, replace = TRUE\n    ) |&gt; \n    summarise(\n        m_sl = mean(Sepal.Length),\n        n = {length(cur_group_rows()) + 30} # 👈 \n    )\n\n\n  \n\n\n\n\n\ncur_column()\n\niris |&gt; \n    as_tibble() |&gt; \n    transmute(\n        across(\n            where(is.numeric),\n            \\(col) {\n                if (stringr::str_detect(cur_column(), \"Sepal\")) { # 👈 \n                    col - mean(col)\n                } else if (stringr::str_detect(cur_column(), \"Petal\")) { # 👈 \n                    (col - mean(col)) / sd(col)\n                } else {\n                    col\n                }\n            }\n        )\n    )\n\n\n  \n\n\n\n\n\n\n\n\n\n5 Conclusion\nI hope they don’t change this soon, it is quite a nice feature (definitely not a bug 😋), assembling the DSL strengths across tidyverse APIs. Even if it is subtle. I still suggest you to attach these functions (through e.g. library() and box::use()) for better maintainability."
  },
  {
    "objectID": "posts/05-pipes/index.html",
    "href": "posts/05-pipes/index.html",
    "title": "How much do you know about pipes?",
    "section": "",
    "text": "I’ll go dive about the history of pipes in R. Pipes have revolutionized the way we write R code, making it more readable and maintainable. But the story of pipes in R is richer than many realize. While most R users are already familiar with magrittr’s %&gt;% or the native R |&gt;, the journey of pipes in R spans multiple packages and years of implementations, each with unique features and use cases.\nIn this post, I’ll be chronological about what I explore in the history and variety of pipes available in R, from the pioneering days to modern implementations.\nBut the question still remains: How much did you really learn about pipes in R?"
  },
  {
    "objectID": "posts/05-pipes/index.html#the-piper-pioneer-2013",
    "href": "posts/05-pipes/index.html#the-piper-pioneer-2013",
    "title": "How much do you know about pipes?",
    "section": "\n4.1 1. The {pipeR} Pioneer (2013)",
    "text": "4.1 1. The {pipeR} Pioneer (2013)\nThe pipeR package by Kun Ren was one of the earliest pipe implementations in R, introducing the %&gt;&gt;% operator.\n\nbox::use(pipeR[`%&gt;&gt;%`])\n\n1:10 %&gt;&gt;%\n    mean() %&gt;&gt;%\n    round(2)\n\n[1] 5.5\n\n\nHere’s the cool part:\n\n\nLambda expressions with parentheses:\n\n1:10 %&gt;&gt;%\n    (mean(.) * 2) %&gt;&gt;%\n    round(2)\n\n[1] 11\n\n\n\nSide effects with continued piping:\n\n::: {.cell}\n\n```{.r .cell-code}\nrnorm(100) %&gt;&gt;%\n    (~ plot(., main = \"Random Normal Values\")) %&gt;&gt;%  # Side effect\n    mean() %&gt;&gt;%\n    round(2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=960}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.07\n```\n\n\n:::\n:::\n\n\n\n\n\n\nWhy it faded\n\n\n\nIt’s not like it vanished from the existence, more like it is superseded by magrittr and took over."
  },
  {
    "objectID": "posts/05-pipes/index.html#the-game-changer-magrittr-pipe-2014",
    "href": "posts/05-pipes/index.html#the-game-changer-magrittr-pipe-2014",
    "title": "How much do you know about pipes?",
    "section": "\n4.2 2. The Game Changer: {magrittr} Pipe (2014)",
    "text": "4.2 2. The Game Changer: {magrittr} Pipe (2014)\nThe magrittr package, created by Stefan Milton Bache and later maintained by Lionel Henry at Posit (formerly RStudio), became the most popular pipe implementation. It was inspired by F#’s pipe-forward operator and Unix pipes.\n\nbox::use(magrittr[`%&gt;%`, `%&lt;&gt;%`, `%T&gt;%`, `%$%`, `%!&gt;%`])\n\nc(1, 2, 3, 4, 5) %&gt;%\n    mean() %&gt;%\n    round(2)\n\n[1] 3\n\n\nDo you know? There are plenty pipe operators in magrittr package, consists of at least 5 operators. Here are the special features:\n\n\nThe classic\nAssignment pipe\nTee pipe\nExposition pipe\nEager pipe\n\n\n\nThe %&gt;% is magrittr’s standard and “lazy” pipe - it doesn’t evaluate arguments until needed, which can affect behavior with certain functions. Lazy evaluation means that the RHS is only computed when its value is required, which optimizes performance but can lead to surprises with side-effect-heavy code.\nTo understand better how %&gt;% works, let’s give a demonstration by applying dot placeholder for non-first arguments:\n\nmtcars %&gt;%\n    lm(mpg ~ cyl, data = .)\n\n\nCall:\nlm(formula = mpg ~ cyl, data = .)\n\nCoefficients:\n(Intercept)          cyl  \n     37.885       -2.876  \n\n\nThe dot (.) acts as a placeholder for the piped value, allowing it to be inserted into any argument position—not just the first. You can also apply multiple placeholders:\n\nmtcars %&gt;% \n    head(5) %&gt;% \n    split(., .$cyl)\n\n$`4`\n            mpg cyl disp hp drat   wt  qsec vs am gear carb\nDatsun 710 22.8   4  108 93 3.85 2.32 18.61  1  1    4    1\n\n$`6`\n                mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4      21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag  21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nHornet 4 Drive 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n\n$`8`\n                   mpg cyl disp  hp drat   wt  qsec vs am gear carb\nHornet Sportabout 18.7   8  360 175 3.15 3.44 17.02  0  0    3    2\n\n\n\n\nThe %&lt;&gt;% operator is invoking reference semantics, where it pipes and assigns the result back to the original variable:\n\nx = 1:5\nx %&lt;&gt;% log() %&gt;% sum()\nx\n\n[1] 4.787492\n\n\nThis is equivalent to x = x %&gt;% log() %&gt;% sum() but more concise. What happened here is we created a side-effect of x. Some pointed it out why it is a problem.\n\n\nThe %T&gt;% “tee” pipe passes the left-hand side value forward, not the output of the right-hand side. Useful for side effects like plotting or printing, where you want to perform an action but continue with the original data:\n\nset.seed(123)\nrnorm(100) %T&gt;% \n    plot(main = \"Values before mean\") %&gt;% \n    mean() %&gt;%\n    round(2)\n\n\n\n\n\n\n\n[1] 0.09\n\n\nThis should be the equivalent:\n{\n    set.seed(123)\n    plot(rnorm(100), main = \"Values before mean\")\n    round(mean(rnorm(100)), 2)\n}\nSo, if you try the following:\n\n1:5 %T&gt;% \n    mean()\n\n[1] 1 2 3 4 5\n\n\nThe %T&gt;% operator discards the output of mean(1:5), and that’s because mean() doesn’t return a side-value effect.\nBy the way, the “tee” name comes from Unix’s tee command, which splits output streams.\n\n\nThe %$% “exposition” pipe exposes the names within the left-hand side object to the right-hand side expression:\n\nmtcars %$%\n    cor(mpg, cyl)\n\n[1] -0.852162\n\n\nThis is equivalent to:\ncor(mtcars$mpg, mtcars$cyl)\nThis is particularly useful with functions that don’t have a data argument.\n\n\n\n\n\n\nWarning\n\n\n\nDo not use %$% operator when LHS is not a a list or data frame with named elements.\n\n\n\n\nThe %!&gt;% operator is the “eager” version of %&gt;% that evaluates arguments immediately. This can matter for functions with non-standard evaluation:\n\n# Standard (lazy) pipe\niris %&gt;% \n    subset(Species == \"setosa\") %&gt;% \n    head(3)\n\n\n  \n\n\n# Eager pipe (forces immediate evaluation)\niris %!&gt;% \n    subset(Species == \"setosa\") %!&gt;% \n    head(3)\n\n\n  \n\n\n\nIn most cases, the difference is subtle, but it can matter for advanced programming.\nTo see the actual difference:\n\n\n%!&gt;%: cat(1) is immediately evaluated (it evaluates from left to right)\n\n\n0 %!&gt;% (\\(x) { cat(1); x }) %!&gt;% (\\(x) cat(2))\n\n12\n\n\n\n\n%&gt;%: Evaluates only cat(2) as the first result is never used\n\n\n0 %&gt;% (\\(x) { cat(1); x }) %&gt;% (\\(x) cat(2))  \n\n2\n\n\nSource: https://stackoverflow.com/questions/76326742/what-are-the-differences-and-use-cases-of-the-five-magrittr-pipes"
  },
  {
    "objectID": "posts/05-pipes/index.html#the-wrapr-dot-arrow-2017",
    "href": "posts/05-pipes/index.html#the-wrapr-dot-arrow-2017",
    "title": "How much do you know about pipes?",
    "section": "\n4.3 3. The {wrapr} Dot Arrow (2017)",
    "text": "4.3 3. The {wrapr} Dot Arrow (2017)\nJohn Mount’s wrapr package provides the %.&gt;% “dot arrow” pipe,\n\nThis pipe requires the dot to always be explicit, which prevents certain subtle bugs and makes code intentions clearer. It’s particularly useful in production code where clarity is paramount."
  },
  {
    "objectID": "posts/05-pipes/index.html#the-bizarro-pipe--.-base-r-2017",
    "href": "posts/05-pipes/index.html#the-bizarro-pipe--.-base-r-2017",
    "title": "How much do you know about pipes?",
    "section": "\n4.4 4. The Bizarro Pipe: ->.; (Base R, ~2017)",
    "text": "4.4 4. The Bizarro Pipe: -&gt;.; (Base R, ~2017)\nI am not sure when this operator released, but there’s a pipe operator in base R: the “Bizarro pipe” (-&gt;.;), that works like %&gt;% and %.&gt;%. It’s not a formal operator but an emergent behavior from combining existing R syntax.\n\n1:10 -&gt;.; \n    mean(.) -&gt;.; \n    round(., 2)\n\n[1] 5.5\n\n\nThe Bizarro pipe works by:\n\nUsing right assignment -&gt; to assign to .\n\nEnding each statement with ; to separate expressions\nThe next line uses . as input\n\nIt’s called “Bizarro” because it uses right-to-left assignment syntax (-&gt;) to create a left-to-right workflow.\nHowever, it has disadvantages (talked in this Stackoverflow discussion):\n\nCreates hidden side-effects (the persistent . variable)\nGoes against R style guides (right assignment and semicolons are discouraged)\nCan lead to subtle bugs if you forget to assign to . at some step\nThe . variable is hidden from ls() and IDE inspectors\nIt’s so pesky, it won’t auto-inden.t\n\nSeriously, the Bizarro pipe is primarily used as a debugging tool to convert %&gt;% pipelines for step-by-step inspection, not for production code."
  },
  {
    "objectID": "posts/05-pipes/index.html#the-native-pipe-r-v4.1-2021",
    "href": "posts/05-pipes/index.html#the-native-pipe-r-v4.1-2021",
    "title": "How much do you know about pipes?",
    "section": "\n4.5 5. The Native Pipe: |> (R v4.1+, 2021)",
    "text": "4.5 5. The Native Pipe: |&gt; (R v4.1+, 2021)\nIn May 2021, R v4.1 introduced the native pipe operator |&gt;, bringing pipe functionality into base R without the need for external packages. This operator is the true operator that was inspired by the pipe-forward operator in F# and the concept of Unix pipes.\n\nc(1, 2, 3, 4, 5) |&gt;\n    mean() |&gt;\n    round(2)\n\n[1] 3\n\n\nThis is too identical to %&gt;% from magrittr with some obvious differences.\n\n4.5.1 Common differences from {magrittr} pipe\nThe placeholder for |&gt; is now applied in R v4.2 and above. For the syntax, it rather uses _, not ..\n\nmtcars |&gt;\n    lm(mpg ~ cyl, data = _)\n\n\nCall:\nlm(formula = mpg ~ cyl, data = mtcars)\n\nCoefficients:\n(Intercept)          cyl  \n     37.885       -2.876  \n\n\nThe native pipe:\n\nIs slightly faster (negligible in often cases, this matters for some cases like running for-loop)\nDoes not support the tee (%T&gt;%), exposition (%$%), or assignment (%&lt;&gt;%) operators\nCannot be used with compound assignment\nIs more strict about valid syntax\n\n4.5.2 Performance comparison\nThe native pipe is clearly faster than the magrittr pipe because native pipe does not add more function calls within its implementation compared to the magrittr pipe.\n\nbench::mark(\n    \"magrittr pipe\" = replicate(10000, 1:100 %&gt;% sum()), \n    \"native R pipe\" = replicate(10000, 1:100 |&gt; sum())\n)\n\n\n  \n\n\n\n\n4.5.3 Pipe-bind operator\nAfter R v4.2, the pipe-bind operator =&gt;, or pipe-binding syntax, allows you to bind the result of the left-hand side (LHS) to a name within the right-hand side (RHS) expression.\nThis feature is, however, disabled by default. You may want to enable it by running the following:\n\nSys.setenv(\"_R_USE_PIPEBIND_\" = TRUE)\n\nAnother options:\n\nPlace this command into .Renviron file (Hint: run usethis::edit_r_environ()):\n\n_R_USE_PIPEBIND_=true\n\nRun this in a command prompt or PowerShell\n\nsetx _R_USE_PIPEBIND_ true\nIf you are in Linux / macOS (bash / zsh):\nexport _R_USE_PIPEBIND_=true\nThen restart R.\nHere’s what it does:\n\nmtcars |&gt; \n    df =&gt; lm(mpg ~ wt, data = df)\n\n\nCall:\nlm(formula = mpg ~ wt, data = df)\n\nCoefficients:\n(Intercept)           wt  \n     37.285       -5.344  \n\nmtcars |&gt; \n    df =&gt; split(df, df$cyl) |&gt; \n    lapply(\\(df) lm(mpg ~ wt, data = df)) |&gt; \n    vapply(\\(mod) summary(mod)$r.squared, numeric(1))\n\n        4         6         8 \n0.5086326 0.4645102 0.4229655 \n\n\nThe df name temporarily exists only inside that RHS expression — not in your global environment. I like this because this is more explicit than . in %&gt;% operator. You can name the LHS result and refer to it directly inside the RHS expression anything you like."
  },
  {
    "objectID": "posts/05-pipes/index.html#non-pipe-alternatives",
    "href": "posts/05-pipes/index.html#non-pipe-alternatives",
    "title": "How much do you know about pipes?",
    "section": "\n4.6 Non-Pipe Alternatives",
    "text": "4.6 Non-Pipe Alternatives\nWhile the above are true pipe operators, it’s worth mentioning that some packages achieve similar left-to-right workflows through different mechanisms.\n\n4.6.1 {data.table} Chaining (2010)\ndata.table uses method chaining with [][] notation. It is NOT a pipe operator in a sense, but achieves a similar left-to-right flow. It behaves differently from the pipe operator — it chains operations within the same [.data.table method, and doesn’t pass values between functions, i.e. the use of placeholders.\nLet’s look at the basic data.table example:\n\nbox::use(data.table[as.data.table, `:=`])\n\ndt = as.data.table(mtcars)\ndt[cyl == 8][order(-mpg)][, .(mpg, cyl, hp)][1:5]\n\n\n  \n\n\n\nDeeper method chaining in data.table with grouping and aggregation:\n\ndt[, log_mpg := log(mpg)][,\n    .(\n        mpg_mean = mean(mpg, na.rm = TRUE), \n        log_mpg_mean = mean(log_mpg, na.rm = TRUE)\n    ), by = cyl\n][\n    order(-mpg_mean, -log_mpg_mean)\n]\n\n\n  \n\n\n\nThis is method chaining, not piping—the key difference is that pipes pass values between different functions, while data.table chains operations within the same [ method.\n\n4.6.2 {zeallot} Multiple Assignment (2018)\nR lacks destructuring (also called “unpacking”) method, just like what you see in other languages, such as Python:\nx, y = 0, 1\nThe zeallot allows destructuring assignment with %&lt;-%. While not exactly a pipe operator to chain the commands, works well in pipe-like workflows.\n\nbox::use(zeallot[`%&lt;-%`])\n\n# Multiple assignment\nc(a, b) %&lt;-% c(1, 2)\nc(a, b)\n\n[1] 1 2\n\n\nDestructuring with computations:\n\nc(mean_val, sd_val, n) %&lt;-% local({\n    set.seed(125)\n    x = rnorm(100)\n    c(mean(x), sd(x), length(x))\n})\n\ncat(glue::glue(\"Mean: {mean_val}, SD: {sd_val} , N: {n}\"), \"\\n\")\n\nMean: 0.100208694251594, SD: 1.06105719788861 , N: 100 \n\n\nWorks with pipe workflows:\n\nset.seed(123)\nc(m, s) %&lt;-% (rnorm(100) %&gt;% {c(mean(.), sd(.))})\nc(m, s)\n\n[1] 0.09040591 0.91281588"
  }
]